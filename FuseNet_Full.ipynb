{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Akhil_Fuse.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d031df1075d746099ca006b9827f4780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e360ec670c5d442aabb807ad951751c8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_610bfe7dabba4d1489cd88e87d38a15a",
              "IPY_MODEL_df18b052fc774b23a905b3f2774935a6"
            ]
          }
        },
        "e360ec670c5d442aabb807ad951751c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "610bfe7dabba4d1489cd88e87d38a15a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_863a1f5b364f44d892a20d7ecc7e1411",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cb372a7133634275b102dd503c5fcb52"
          }
        },
        "df18b052fc774b23a905b3f2774935a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_873a3bc0a2bc4b8e860df07709d6d918",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:17&lt;00:00, 5.94MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7160069ccbde4231a6fc86009bef0259"
          }
        },
        "863a1f5b364f44d892a20d7ecc7e1411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cb372a7133634275b102dd503c5fcb52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "873a3bc0a2bc4b8e860df07709d6d918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7160069ccbde4231a6fc86009bef0259": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tgqAhYHvNg2"
      },
      "source": [
        "# Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA580dbFbyEn",
        "outputId": "45920f54-8c35-4696-d604-6d7ac443c6cf"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Dec 12 17:05:47 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOC44nKYDOn8"
      },
      "source": [
        "# Run this command once and restart the runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhZffaB6C7Z7",
        "outputId": "4fb7a072-b066-4365-d726-2e6211d0606a"
      },
      "source": [
        "!pip install av"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting av\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/62/9a992be76f8e13ce0e3a24a838191b546805545116f9fc869bd11bd21b5f/av-8.0.2-cp36-cp36m-manylinux2010_x86_64.whl (36.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36.9MB 169kB/s \n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-8.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ati1HrCZNHE_"
      },
      "source": [
        "# HMDB51 + DataLoader, Spatial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQwd2BDyLFxT"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "from torchvision.datasets.utils import list_dir\n",
        "from torchvision.datasets.folder import make_dataset\n",
        "from torchvision.datasets.video_utils import VideoClips\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "\n",
        "class HMDB51(VisionDataset):\n",
        "    \"\"\"\n",
        "    Internally, it uses a VideoClips object to handle clip creation.\n",
        "    Args:\n",
        "        root (string): Root directory of the HMDB51 Dataset.\n",
        "        frames_per_clip (int): Number of frames in a clip.\n",
        "        step_between_clips (int): Number of frames between each clip.\n",
        "        train (bool, optional): If ``True``, creates a dataset from the train split,\n",
        "            otherwise from the ``test`` split.\n",
        "        transform (callable, optional): A function/transform that takes in a TxHxWxC video\n",
        "            and returns a transformed version.\n",
        "    Returns:\n",
        "        video (Tensor[T, H, W, C]): the `T` video frames\n",
        "        audio(Tensor[K, L]): the audio frames, where `K` is the number of channels\n",
        "            and `L` is the number of points\n",
        "        label (int): class of the video clip\n",
        "    \"\"\"\n",
        "    def __init__(self, root=\"\"):\n",
        "        super(HMDB51, self).__init__(root)\n",
        "\n",
        "    def init_data(self, root, frames_per_clip, step_between_clips=1,\n",
        "                 frame_rate=None, train=True, transform=None,\n",
        "                 _precomputed_metadata=None, num_workers=1, _video_width=0,\n",
        "                 _video_height=0, _video_min_dimension=0, _audio_samples=0):\n",
        "        super(HMDB51, self).__init__(root)\n",
        "        extensions = ('avi',)\n",
        "        if train:\n",
        "            root = root + \"/train\"\n",
        "        else:\n",
        "            root = root + \"/test\"\n",
        "        classes = sorted(list_dir(root))\n",
        "        class_to_idx = {class_: i for (i, class_) in enumerate(classes)}\n",
        "        print(class_to_idx)\n",
        "        self.samples = []\n",
        "        for target_class in sorted(class_to_idx.keys()):\n",
        "            class_index = class_to_idx[target_class]\n",
        "            target_dir = os.path.join(root, target_class)\n",
        "            for root_curr, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
        "                for fname in sorted(fnames):\n",
        "                    path = os.path.join(root_curr, fname)\n",
        "                    if os.path.isfile(path):\n",
        "                        item = path, class_index\n",
        "                        self.samples.append(item)\n",
        "\n",
        "        video_paths = [path for (path, _) in self.samples]\n",
        "        video_clips = VideoClips(\n",
        "            video_paths,\n",
        "            frames_per_clip,\n",
        "            step_between_clips,\n",
        "            frame_rate,\n",
        "            _precomputed_metadata,\n",
        "            num_workers=num_workers,\n",
        "            _video_width=_video_width,\n",
        "            _video_height=_video_height,\n",
        "            _video_min_dimension=_video_min_dimension,\n",
        "            _audio_samples=_audio_samples,\n",
        "        )\n",
        "        self.train = train\n",
        "        self.classes = classes\n",
        "        self.video_clips_metadata = video_clips.metadata\n",
        "        self.indices = self.get_indices(video_paths)\n",
        "        self.video_clips = video_clips.subset(self.indices)\n",
        "        self.transform = transform\n",
        "\n",
        "    @property\n",
        "    def metadata(self):\n",
        "        return self.video_clips_metadata\n",
        "\n",
        "    def get_indices(self, video_list):\n",
        "        indices = []\n",
        "        for video_index, video_path in enumerate(video_list):\n",
        "            indices.append(video_index)\n",
        "        return indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.video_clips.num_clips()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video, _, _, video_idx = self.video_clips.get_clip(idx)\n",
        "        sample_index = self.indices[video_idx]\n",
        "        _, class_index = self.samples[sample_index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            video = video.permute(0, 3, 1, 2)\n",
        "            video = self.transform(video)\n",
        "            video = video.permute(0, 2, 3, 1)\n",
        "\n",
        "        return video, class_index, sample_index\n",
        "\n",
        "    def state_dict(self):\n",
        "        state = {\"video_clips\": self.video_clips,\n",
        "                 \"indices\": self.indices,\n",
        "                 \"samples\": self.samples,\n",
        "                 \"transform\": self.transform,\n",
        "                 \"metadata\": self.video_clips_metadata}\n",
        "\n",
        "        return state\n",
        "\n",
        "    def load_state_dict(self, state):\n",
        "        self.video_clips = state[\"video_clips\"]\n",
        "        self.indices = state[\"indices\"]\n",
        "        self.samples = state[\"samples\"]\n",
        "        self.transform = state[\"transform\"]\n",
        "        self.video_clips_metadata = state[\"metadata\"]\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class BuildDataLoader(torch.utils.data.DataLoader):\n",
        "    def __init__(self, dataset, batch_size, shuffle, num_workers):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.num_workers = num_workers\n",
        "        self.w_frame = 224\n",
        "        self.h_frame = 224\n",
        "\n",
        "    def collect_fn(self, batch):\n",
        "        video_list = []\n",
        "        label_list = []\n",
        "        index_list = []\n",
        "\n",
        "        for video, cl_index, s_index in batch:\n",
        "            video_list.append(video)\n",
        "            label_list.append(cl_index)\n",
        "            index_list.append(s_index)\n",
        "\n",
        "        data = {\"videos\": torch.stack(video_list),\n",
        "                \"labels\": label_list,\n",
        "                \"indexes\": index_list\n",
        "                }\n",
        "\n",
        "        return data\n",
        "\n",
        "    def loader(self):\n",
        "        return DataLoader(self.dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=self.shuffle,\n",
        "                          num_workers=self.num_workers,\n",
        "                          collate_fn=self.collect_fn)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwpGXLdtuq8i"
      },
      "source": [
        "# Mount drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6nsoL9Kuqil",
        "outputId": "01255e8f-83f4-4d49-fe2b-8e044721e681"
      },
      "source": [
        "import os\n",
        "import io\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount google drive\n",
        "DRIVE_MOUNT='/content/drive'\n",
        "\n",
        "drive.mount(DRIVE_MOUNT)\n",
        "\n",
        "\n",
        "# create folder to write data to\n",
        "DATA_FOLDER = os.path.join(DRIVE_MOUNT, 'Shared drives', 'CIS680 Final Project', 'data')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n37IkHXBt2sn"
      },
      "source": [
        "# Load Dataset and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tpb1CP4et6AW",
        "outputId": "78f95358-b369-41af-ce56-83ef96b4e08a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = HMDB51(DATA_FOLDER)\n",
        "test_dataset = HMDB51(DATA_FOLDER)\n",
        "\n",
        "train_dataset.load_state_dict(torch.load(DATA_FOLDER + \"/train_dataset_3.pt\"))\n",
        "test_dataset.load_state_dict(torch.load(DATA_FOLDER + \"/test_dataset_3.pt\"))\n",
        "\n",
        "batch_size = 64 # reduced batch size for fuse since Cuda runs out of memory for 128\n",
        "train_build_loader = BuildDataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "train_loader = train_build_loader.loader()\n",
        "test_build_loader = BuildDataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = test_build_loader.loader()\n",
        "\n",
        "print(\"Length of train_loader: {}\".format(len(train_loader)))\n",
        "print(\"Length of test_loader: {}\".format(len(test_loader)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train_loader: 7536\n",
            "Length of test_loader: 2208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo6FpL-ddpSI"
      },
      "source": [
        "# Spatial Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8J0AEPhdqg8"
      },
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torch import nn, Tensor\n",
        "import random\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class SpatialStream(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 device='cuda',\n",
        "                 num_classes=51,\n",
        "                 dropout_probability=0.5,\n",
        "                 train_resnet=True):\n",
        "\n",
        "        # Initialize the stream layers\n",
        "        super(SpatialStream, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Spatial Pretrained Resnet-50\n",
        "        self.spatial = models.resnet50(pretrained=True)\n",
        "        for param in self.spatial.parameters():\n",
        "            param.requires_grad = train_resnet  # False: Freezes the weights of the pre-trained model\n",
        "\n",
        "        # Add FC layers to Spatial Resnet-50\n",
        "        self.spatial.fc = nn.Sequential(nn.Linear(2048, 1024),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(p=dropout_probability),\n",
        "                                nn.Linear(1024, self.num_classes),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(p=dropout_probability),\n",
        "                                nn.Softmax())\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.spatial(X)\n",
        "\n",
        "    def compute_loss(self, output, labels):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(output, labels)\n",
        "        return loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR8qQX0YT83S"
      },
      "source": [
        "# Temporal Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlaLhE8VT8U0"
      },
      "source": [
        "class TemporalStream(torch.nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 device='cuda',\n",
        "                 num_classes=51,\n",
        "                 dropout_probability=0.5):\n",
        "\n",
        "        # Initialize the stream layers\n",
        "        super(TemporalStream, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Temporal Pretrained Resnet-50\n",
        "        self.temporal = models.resnet50(pretrained=True)\n",
        "        for param in self.temporal.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Add FC layers to Temporal Resnet-50\n",
        "        self.temporal.fc = nn.Sequential(nn.Linear(2048, 1024),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(p=dropout_probability),\n",
        "                                nn.Linear(1024, self.num_classes),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(p=dropout_probability),\n",
        "                                nn.Softmax())\n",
        "        \n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.temporal(X)\n",
        "        return X\n",
        "\n",
        "    def compute_loss(self, output, target):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(output, target)\n",
        "        return loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BkCNZhhT_70"
      },
      "source": [
        "# Fuse Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeUFQULLT_Kl"
      },
      "source": [
        "class FuseNET(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 device='cuda',\n",
        "                 num_classes=51):\n",
        "\n",
        "        # Initialize the FuseNet FC layers\n",
        "        super(FuseNET, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.fc1 = nn.Linear(102, 204)\n",
        "        self.output = nn.Linear(204, self.num_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.fc1(X)\n",
        "        X = self.output(X)\n",
        "        return X\n",
        "\n",
        "    def compute_loss(self, output, labels):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(output, labels)\n",
        "        return loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pEvnrSVeftj"
      },
      "source": [
        "# Train & Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LIHVXrkzK1N"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def getSG3I(videos):\n",
        "    bz = videos.size(0)\n",
        "\n",
        "    frame_list_batch = []\n",
        "\n",
        "    for b in range(bz):\n",
        "        images = videos[b]\n",
        "\n",
        "        w = images[0].size(0)\n",
        "        h = images[0].size(1)\n",
        "        num_frames = images.size(0)\n",
        "        frame_list = []\n",
        "\n",
        "        for i in range(0, num_frames):\n",
        "            frame_list.append(cv.cvtColor(images[i].numpy(), cv.COLOR_BGR2GRAY))\n",
        "\n",
        "        frame_list_batch.append(np.stack(frame_list, axis=-1))\n",
        "\n",
        "    SG3I = np.stack(frame_list_batch, axis=0)\n",
        "\n",
        "    return torch.Tensor(SG3I)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggkzqmwzefQq"
      },
      "source": [
        "  def train(epoch):\n",
        "    spatial.eval()\n",
        "    temporal.eval()\n",
        "    fused.train()\n",
        "\n",
        "    counter = 0\n",
        "    train_loss = 0\n",
        "    log_interval = 100\n",
        "    save_interval = 250\n",
        "\n",
        "    epoch_loss = []\n",
        "    log_int_loss = 0\n",
        "    for iter, data in enumerate(train_loader, 7499):\n",
        "        print(\"iter = \", iter)\n",
        "        if iter <= 7499:\n",
        "          continue\n",
        "        if iter > 7536:\n",
        "          break\n",
        "\n",
        "        videos = data[\"videos\"]\n",
        "        labels = torch.tensor(data[\"labels\"])\n",
        "        indexes = data[\"indexes\"]\n",
        "        \n",
        "        SG3I = getSG3I(videos)\n",
        "        SG3I = SG3I.permute(0,3,1,2)\n",
        "        SG3I = SG3I.to(device)\n",
        "\n",
        "        videos = videos.type(torch.FloatTensor)\n",
        "        videos = videos.to(device)\n",
        "        # labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # spatial\n",
        "        spatial_input = videos[:,0,:,:].permute(0,3,2,1)\n",
        "        spatial_output = spatial(spatial_input)\n",
        "\n",
        "        # temporal        \n",
        "        temporal_output = temporal(SG3I)\n",
        "        \n",
        "        # fused\n",
        "        fused_input = torch.hstack((spatial_output, temporal_output))\n",
        "        fused_output = fused(fused_input)\n",
        "\n",
        "        fused_output = fused_output.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # calculate losses\n",
        "        loss = fused.compute_loss(fused_output, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Logging Interval\n",
        "        log_int_loss += loss.item()\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "        if counter == 0:\n",
        "            print('Epoch: ', epoch, ', Batch: ', iter, ', loss avg over log interval: ', log_int_loss)\n",
        "            train_loss_list.append(train_loss / (iter + 1) * batch_size)\n",
        "            train_counter.append((iter + 1) * batch_size + epoch * len(train_loader.dataset))\n",
        "            log_int_loss = 0\n",
        "        elif counter % log_interval == log_interval - 1:\n",
        "            print('Epoch: ', epoch, ', Batch: ', iter, ', loss avg over log interval: ', log_int_loss / log_interval)\n",
        "            train_loss_list.append(train_loss / (iter + 1) * batch_size)\n",
        "            train_counter.append((iter + 1) * batch_size + epoch * len(train_loader.dataset))\n",
        "            log_int_loss = 0\n",
        "\n",
        "        if counter % save_interval == save_interval - 1:\n",
        "            print('saving model')\n",
        "            save_path = os.path.join(EPOCH_SAVE_PREFIX, 'fused_epoch' + str(epoch) + '_iter_' + str(counter))\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'train_total_loss_list': train_loss_list,\n",
        "                'epoch_total_loss_list': epoch_loss_list,\n",
        "                'test_loss_list': test_loss_list,\n",
        "                'train_counter': train_counter,\n",
        "                'accuracy_list': accuracy_list,\n",
        "                'model_state_dict': fused.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict()\n",
        "            }, save_path)\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "\n",
        "    avg_loss = sum(epoch_loss) / len(epoch_loss)\n",
        "    epoch_loss_list.append(avg_loss)\n",
        "    print('Epoch: ', epoch, ', avg total loss: ', avg_loss)\n",
        "\n",
        "def test():\n",
        "    fused.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # Data Loop\n",
        "    with torch.no_grad():\n",
        "        for iter, data in enumerate(test_loader, 0):\n",
        "            videos = data[\"videos\"]\n",
        "            labels = torch.tensor(data[\"labels\"])\n",
        "            indexes = data[\"indexes\"]\n",
        "            \n",
        "            SG3I = getSG3I(videos)\n",
        "            SG3I = SG3I.permute(0,3,1,2)\n",
        "            SG3I = SG3I.to(device)\n",
        "\n",
        "            videos = videos.type(torch.FloatTensor)\n",
        "            videos = videos.to(device)\n",
        "            # labels = labels.to(device)\n",
        "\n",
        "            # spatial\n",
        "            spatial_input = videos[:,0,:,:].permute(0,3,2,1)\n",
        "            spatial_output = spatial(spatial_input)\n",
        "\n",
        "            # temporal        \n",
        "            temporal_output = temporal(SG3I)\n",
        "            \n",
        "            # fused\n",
        "            fused_input = torch.hstack((spatial_output, temporal_output))\n",
        "            fused_output = fused(fused_input)\n",
        "\n",
        "            fused_output = fused_output.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # calculate losses\n",
        "            loss = fused.compute_loss(fused_output, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # calculate number of correct predictions in batch\n",
        "            correct += sum(torch.argmax(fused_output,1) == labels).item()\n",
        "            if iter % 100 == 0:\n",
        "                print (\"iter  \", iter)\n",
        "                print(\"accuracy so far = \", correct / ((iter + 1) * len(labels)))\n",
        "\n",
        "    # Log\n",
        "    test_loss_list.append(test_loss / len(test_loader.dataset))\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "    accuracy_list.append(accuracy)\n",
        "    print('Avg Validation Loss: ', test_loss / len(test_loader.dataset))\n",
        "    print('Accuracy: ', accuracy)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxNfXCLLYcbW"
      },
      "source": [
        "# Main Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642,
          "referenced_widgets": [
            "d031df1075d746099ca006b9827f4780",
            "e360ec670c5d442aabb807ad951751c8",
            "610bfe7dabba4d1489cd88e87d38a15a",
            "df18b052fc774b23a905b3f2774935a6",
            "863a1f5b364f44d892a20d7ecc7e1411",
            "cb372a7133634275b102dd503c5fcb52",
            "873a3bc0a2bc4b8e860df07709d6d918",
            "7160069ccbde4231a6fc86009bef0259"
          ]
        },
        "id": "SiW7i49DYbwc",
        "outputId": "3c8a32b9-eef6-43d9-c0eb-ff0980fdea73"
      },
      "source": [
        "EPOCH_SAVE_PREFIX = '/content/drive/Shared drives/CIS680 Final Project/models/fused/'\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "torch.random.manual_seed(1)\n",
        "\n",
        "# load trained spatial\n",
        "spatial = SpatialStream()\n",
        "spatial.to(device)\n",
        "spatial_network_path = '/content/drive/Shared drives/CIS680 Final Project/models/spatial/spatial_epoch10'\n",
        "checkpoint_spatial = torch.load(spatial_network_path)\n",
        "spatial.load_state_dict(checkpoint_spatial['model_state_dict'])\n",
        "\n",
        "# load trained temporal\n",
        "temporal = TemporalStream()\n",
        "temporal.to(device)\n",
        "temporal_network_path = '/content/drive/Shared drives/CIS680 Final Project/models/temporal_SG3I/temporal_epoch8'\n",
        "checkpoint_temporal = torch.load(temporal_network_path)\n",
        "temporal.load_state_dict(checkpoint_temporal['model_state_dict'])\n",
        "\n",
        "\n",
        "# initialize fused model\n",
        "learning_rate = 0.001\n",
        "fused = FuseNET()\n",
        "fused.to(device)\n",
        "optimizer = optim.SGD(fused.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "# Epochs\n",
        "num_epochs = 50\n",
        "\n",
        "# Logging setup: train\n",
        "train_loss_list = []\n",
        "epoch_loss_list = []\n",
        "train_counter = []\n",
        "\n",
        "# Logging setup: test\n",
        "test_loss_list = []\n",
        "accuracy_list = []\n",
        "epoch_list = np.arange(num_epochs)\n",
        "\n",
        "# epoch loop\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Train & Validate\n",
        "    train(epoch)\n",
        "    test()\n",
        "\n",
        "    # Save Model Version\n",
        "    save_path = os.path.join(EPOCH_SAVE_PREFIX, 'fused_epoch' + str(epoch))\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'train_total_loss_list': train_loss_list,\n",
        "        'epoch_total_loss_list': epoch_loss_list,\n",
        "        'test_loss_list': test_loss_list,\n",
        "        'train_counter': train_counter,\n",
        "        'accuracy_list': accuracy_list,\n",
        "        'model_state_dict': fused.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, save_path)\n",
        "\n",
        "    print(\"Epoch %d/%d Completed\" % (epoch, num_epochs - 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d031df1075d746099ca006b9827f4780",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:117: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
            "  + \"follow-up version. Please use pts_unit 'sec'.\"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0 , Batch:  0 , loss avg over log interval:  3.9206459522247314\n",
            "Epoch:  0 , Batch:  99 , loss avg over log interval:  3.8703181767463684\n",
            "Epoch:  0 , Batch:  199 , loss avg over log interval:  3.872889759540558\n",
            "saving model\n",
            "Epoch:  0 , Batch:  299 , loss avg over log interval:  3.8393946290016174\n",
            "Epoch:  0 , Batch:  399 , loss avg over log interval:  3.8074916434288024\n",
            "Epoch:  0 , Batch:  499 , loss avg over log interval:  3.7731434655189515\n",
            "saving model\n",
            "Epoch:  0 , Batch:  599 , loss avg over log interval:  3.7234731125831604\n",
            "Epoch:  0 , Batch:  699 , loss avg over log interval:  3.695622055530548\n",
            "saving model\n",
            "Epoch:  0 , Batch:  799 , loss avg over log interval:  3.6621727991104125\n",
            "Epoch:  0 , Batch:  899 , loss avg over log interval:  3.634461441040039\n",
            "Epoch:  0 , Batch:  999 , loss avg over log interval:  3.5999487257003784\n",
            "saving model\n",
            "Epoch:  0 , Batch:  1099 , loss avg over log interval:  3.5693886423110963\n",
            "Epoch:  0 , Batch:  1199 , loss avg over log interval:  3.5482602977752684\n",
            "saving model\n",
            "Epoch:  0 , Batch:  1299 , loss avg over log interval:  3.5301318621635436\n",
            "Epoch:  0 , Batch:  1399 , loss avg over log interval:  3.4823023462295533\n",
            "Epoch:  0 , Batch:  1499 , loss avg over log interval:  3.460193908214569\n",
            "saving model\n",
            "Epoch:  0 , Batch:  1599 , loss avg over log interval:  3.4484406876564027\n",
            "Epoch:  0 , Batch:  1699 , loss avg over log interval:  3.4257140588760375\n",
            "saving model\n",
            "Epoch:  0 , Batch:  1799 , loss avg over log interval:  3.3887492632865905\n",
            "Epoch:  0 , Batch:  1899 , loss avg over log interval:  3.3411956524848936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDmKw9pMH3NJ"
      },
      "source": [
        "# Resume Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UlgNih0H2DD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3961994f-f135-4eaa-acef-6d093a99ba2c"
      },
      "source": [
        "EPOCH_SAVE_PREFIX = '/content/drive/Shared drives/CIS680 Final Project/models/fused/'\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "torch.random.manual_seed(1)\n",
        "\n",
        "# load trained spatial\n",
        "spatial = SpatialStream()\n",
        "spatial.to(device)\n",
        "spatial_network_path = '/content/drive/Shared drives/CIS680 Final Project/models/spatial/spatial_epoch10'\n",
        "checkpoint_spatial = torch.load(spatial_network_path)\n",
        "spatial.load_state_dict(checkpoint_spatial['model_state_dict'])\n",
        "\n",
        "# load trained temporal\n",
        "temporal = TemporalStream()\n",
        "temporal.to(device)\n",
        "temporal_network_path = '/content/drive/Shared drives/CIS680 Final Project/models/temporal_SG3I/temporal_epoch8'\n",
        "checkpoint_temporal = torch.load(temporal_network_path)\n",
        "temporal.load_state_dict(checkpoint_temporal['model_state_dict'])\n",
        "\n",
        "\n",
        "# FuseNet model\n",
        "learning_rate = 0.001\n",
        "fused = FuseNET()\n",
        "fused.to(device)\n",
        "optimizer = optim.SGD(fused.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "# Epochs\n",
        "num_epochs = 50\n",
        "epoch_list = np.arange(num_epochs)\n",
        "\n",
        "\n",
        "# Load FuseNet model\n",
        "fused_network_path = '/content/drive/Shared drives/CIS680 Final Project/models/fused/fused_epoch1_iter_7499'\n",
        "checkpoint_fused = torch.load(fused_network_path)\n",
        "fused.load_state_dict(checkpoint_fused['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint_fused['optimizer_state_dict'])\n",
        "last_epoch = checkpoint_fused['epoch']\n",
        "\n",
        "# Logging setup\n",
        "train_loss_list = checkpoint_fused['train_total_loss_list']\n",
        "epoch_loss_list = checkpoint_fused['epoch_total_loss_list']\n",
        "test_loss_list = checkpoint_fused['test_loss_list']\n",
        "accuracy_list = checkpoint_fused['accuracy_list']\n",
        "train_counter = checkpoint_fused['train_counter']\n",
        "\n",
        "\n",
        "# epoch loop\n",
        "for epoch in range(1, num_epochs):\n",
        "\n",
        "    # Train & Validate\n",
        "    train(epoch)\n",
        "    test()\n",
        "\n",
        "    # Save Model Version\n",
        "    save_path = os.path.join(EPOCH_SAVE_PREFIX, 'fused_epoch' + str(epoch))\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'train_total_loss_list': train_loss_list,\n",
        "        'epoch_total_loss_list': epoch_loss_list,\n",
        "        'test_loss_list': test_loss_list,\n",
        "        'train_counter': train_counter,\n",
        "        'accuracy_list': accuracy_list,\n",
        "        'model_state_dict': fused.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, save_path)\n",
        "\n",
        "    print(\"Epoch %d/%d Completed\" % (epoch, num_epochs - 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:117: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
            "  + \"follow-up version. Please use pts_unit 'sec'.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter =  7499\n",
            "iter =  7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1 , Batch:  7500 , loss avg over log interval:  1.4026941061019897\n",
            "iter =  7501\n",
            "iter =  7502\n",
            "iter =  7503\n",
            "iter =  7504\n",
            "iter =  7505\n",
            "iter =  7506\n",
            "iter =  7507\n",
            "iter =  7508\n",
            "iter =  7509\n",
            "iter =  7510\n",
            "iter =  7511\n",
            "iter =  7512\n",
            "iter =  7513\n",
            "iter =  7514\n",
            "iter =  7515\n",
            "iter =  7516\n",
            "iter =  7517\n",
            "iter =  7518\n",
            "iter =  7519\n",
            "iter =  7520\n",
            "iter =  7521\n",
            "iter =  7522\n",
            "iter =  7523\n",
            "iter =  7524\n",
            "iter =  7525\n",
            "iter =  7526\n",
            "iter =  7527\n",
            "iter =  7528\n",
            "iter =  7529\n",
            "iter =  7530\n",
            "iter =  7531\n",
            "iter =  7532\n",
            "iter =  7533\n",
            "iter =  7534\n",
            "iter =  7535\n",
            "iter =  7536\n",
            "iter =  7537\n",
            "Epoch:  1 , avg total loss:  1.398154616355896\n",
            "iter   0\n",
            "accuracy so far =  0.921875\n",
            "iter   100\n",
            "accuracy so far =  0.5323329207920792\n",
            "iter   200\n",
            "accuracy so far =  0.42319651741293535\n",
            "iter   300\n",
            "accuracy so far =  0.5083056478405316\n",
            "iter   400\n",
            "accuracy so far =  0.49423316708229426\n",
            "iter   500\n",
            "accuracy so far =  0.4937624750499002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy2dqXCRNr7g"
      },
      "source": [
        "# Calculate Accuracy of a Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-Nbs6zlNy_0"
      },
      "source": [
        "# Device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# model\n",
        "learning_rate = 0.001\n",
        "spatial = SpatialStream()\n",
        "spatial.to(device)\n",
        "optimizer = optim.SGD(spatial.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "# LOAD NETWORK\n",
        "network_path = '/content/drive/Shared drives/CIS680 Final Project/models/spatial/spatial_epoch0'\n",
        "checkpoint = torch.load(network_path)\n",
        "spatial.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "last_epoch = checkpoint['epoch']\n",
        "\n",
        "# Logging setup: train\n",
        "train_loss_list = checkpoint['train_total_loss_list']\n",
        "epoch_loss_list = checkpoint['epoch_total_loss_list']\n",
        "test_loss_list = checkpoint['test_loss_list']\n",
        "train_counter = checkpoint['train_counter']\n",
        "\n",
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}