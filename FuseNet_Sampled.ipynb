{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rahul FuseNet_Sampled.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tgqAhYHvNg2"
      },
      "source": [
        "# Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA580dbFbyEn",
        "outputId": "387e77bf-8eab-4bb1-8250-fdff1d32f1dc"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Dec 18 17:51:59 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOC44nKYDOn8"
      },
      "source": [
        "# Run this command once and restart the runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhZffaB6C7Z7",
        "outputId": "c756cd60-feed-4654-cdeb-5a1165a85851"
      },
      "source": [
        "!pip install av"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting av\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/62/9a992be76f8e13ce0e3a24a838191b546805545116f9fc869bd11bd21b5f/av-8.0.2-cp36-cp36m-manylinux2010_x86_64.whl (36.9MB)\n",
            "\u001b[K     |████████████████████████████████| 36.9MB 138kB/s \n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-8.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwpGXLdtuq8i"
      },
      "source": [
        "# Mount drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6nsoL9Kuqil",
        "outputId": "678f726d-c8e9-4e87-c883-483e7fb105ec"
      },
      "source": [
        "import os\n",
        "import io\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount google drive\n",
        "DRIVE_MOUNT='/content/drive'\n",
        "\n",
        "drive.mount(DRIVE_MOUNT)\n",
        "\n",
        "\n",
        "# create folder to write data to\n",
        "DATA_FOLDER = os.path.join(DRIVE_MOUNT, 'Shared drives', 'CIS680 Final Project', 'data')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwe7Gmz2EHZp"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4hrmV77EGvg",
        "outputId": "a6385a69-5cc9-48c4-9351-f3bd8452e85c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torch import nn, Tensor\n",
        "# from dataset import *\n",
        "import random\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "train_size = 138\n",
        "test_size = 41\n",
        "\n",
        "train_path = DATA_FOLDER + \"/dataset_3/train/train_b\"\n",
        "test_path = DATA_FOLDER + \"/dataset_3/test/test_b\"\n",
        "\n",
        "train_loader = []\n",
        "test_loader = []\n",
        "\n",
        "for batch in range(train_size):\n",
        "    batch_path = train_path + str(batch) + \".pt\"\n",
        "    train_loader.append(torch.load(batch_path))\n",
        "    if batch % 10 == 0:\n",
        "        print(\"train: \" + str(batch) + \"/\" + str(train_size))\n",
        "\n",
        "for batch in range(test_size):\n",
        "    batch_path = test_path + str(batch) + \".pt\"\n",
        "    test_loader.append(torch.load(batch_path))\n",
        "    if batch % 10 == 0:\n",
        "        print(\"test: \" + str(batch) + \"/\" + str(test_size))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: 0/138\n",
            "train: 10/138\n",
            "train: 20/138\n",
            "train: 30/138\n",
            "train: 40/138\n",
            "train: 50/138\n",
            "train: 60/138\n",
            "train: 70/138\n",
            "train: 80/138\n",
            "train: 90/138\n",
            "train: 100/138\n",
            "train: 110/138\n",
            "train: 120/138\n",
            "train: 130/138\n",
            "test: 0/41\n",
            "test: 10/41\n",
            "test: 20/41\n",
            "test: 30/41\n",
            "test: 40/41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo6FpL-ddpSI"
      },
      "source": [
        "# Spatial Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8J0AEPhdqg8"
      },
      "source": [
        "class SpatialStream(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 device='cuda',\n",
        "                 num_classes=51,\n",
        "                 dropout_probability=0.5,\n",
        "                 train_resnet=True):\n",
        "\n",
        "        # Initialize the stream layers\n",
        "        super(SpatialStream, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Spatial Backbone\n",
        "        self.spatial = models.resnet50(pretrained=True)\n",
        "        for param in self.spatial.parameters():\n",
        "            param.requires_grad = train_resnet  # False: Freezes the weights of the pre-trained model\n",
        "\n",
        "        # Add to Spatial Backbone\n",
        "        self.spatial.fc = nn.Sequential(nn.Linear(2048, 1024),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(p=dropout_probability),\n",
        "                                nn.Linear(1024, self.num_classes),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(p=dropout_probability),\n",
        "                                nn.Softmax())\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.spatial(X)\n",
        "\n",
        "    def compute_loss(self, output, labels):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        # labels = torch.tensor(labels)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        return loss"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR8qQX0YT83S"
      },
      "source": [
        "# Temporal Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlaLhE8VT8U0"
      },
      "source": [
        "class TemporalStream(torch.nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 device='cuda',\n",
        "                 num_classes=51,\n",
        "                 dropout_probability=0.5):\n",
        "\n",
        "        # Initialize the stream layers\n",
        "        super(TemporalStream, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Backbone\n",
        "        self.temporal = models.resnet50(pretrained=True)\n",
        "        for param in self.temporal.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        #self.temporal.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3)\n",
        "\n",
        "        self.temporal.fc = nn.Sequential(nn.Linear(2048, 1024),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(p=dropout_probability),\n",
        "                                nn.Linear(1024, self.num_classes),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(p=dropout_probability),\n",
        "                                nn.Softmax())\n",
        "        \n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.temporal(X)\n",
        "        return X\n",
        "\n",
        "    def compute_loss(self, output, target):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(output, target)\n",
        "        return loss"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BkCNZhhT_70"
      },
      "source": [
        "# Fuse Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeUFQULLT_Kl"
      },
      "source": [
        "class FuseNET(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 device='cuda',\n",
        "                 num_classes=51):\n",
        "\n",
        "        # Initialize the stream layers\n",
        "        super(FuseNET, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.fc1 = nn.Linear(102, self.num_classes, bias=False)\n",
        "        nn.init.constant_(self.fc1.weight, 0.5)\n",
        "        #self.output = nn.Linear(204, self.num_classes)\n",
        "        \n",
        "        self.softmax = nn.Softmax()\n",
        "        #self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.fc1(X)\n",
        "        #X = self.relu(X)\n",
        "        #X = self.output(X)\n",
        "        X = self.softmax(X)\n",
        "        return X\n",
        "\n",
        "    def compute_loss(self, output, labels):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        # labels = torch.tensor(labels)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        return loss"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pEvnrSVeftj"
      },
      "source": [
        "# Video Frame Stacking (SG3I)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LIHVXrkzK1N"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2 as cv\n",
        "\n",
        "def getSG3I(videos):\n",
        "    bz = videos.size(0)\n",
        "\n",
        "    frame_list_batch = []\n",
        "\n",
        "    for b in range(bz):\n",
        "        images = videos[b]\n",
        "\n",
        "        w = images[0].size(0)\n",
        "        h = images[0].size(1)\n",
        "        num_frames = images.size(0)\n",
        "        frame_list = []\n",
        "\n",
        "        for i in range(0, num_frames):\n",
        "            frame_list.append(cv.cvtColor(images[i].numpy(), cv.COLOR_BGR2GRAY))\n",
        "\n",
        "        frame_list_batch.append(np.stack(frame_list, axis=-1))\n",
        "\n",
        "    SG3I = np.stack(frame_list_batch, axis=0)\n",
        "    #cv2_imshow(SG3I[0])\n",
        "\n",
        "    return torch.Tensor(SG3I)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU4tMD-_E4Aw"
      },
      "source": [
        "# Train and Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggkzqmwzefQq"
      },
      "source": [
        "  import random\n",
        "  \n",
        "  def train(epoch):\n",
        "    spatial.eval()\n",
        "    temporal.eval()\n",
        "    fused.train()\n",
        "\n",
        "    batch_iter = [0,64]\n",
        "    counter = 0\n",
        "    train_loss = 0\n",
        "    log_interval = 100\n",
        "    save_interval = 250\n",
        "\n",
        "    epoch_loss = []\n",
        "    log_int_loss = 0\n",
        "    for iter, data in enumerate(train_loader, 0):\n",
        "        for i in batch_iter:\n",
        "\n",
        "          videos = data[\"videos\"][0+i:i+64]\n",
        "          labels = torch.tensor(data[\"labels\"][0+i:i+64])\n",
        "          indexes = data[\"indexes\"][0+i:i+64]\n",
        "          \n",
        "          SG3I = getSG3I(videos)\n",
        "          SG3I = SG3I.permute(0,3,1,2)\n",
        "          SG3I = SG3I.to(device)\n",
        "\n",
        "          videos = videos.type(torch.FloatTensor)\n",
        "          videos = videos.to(device)\n",
        "          # labels = labels.to(device)\n",
        "\n",
        "          \n",
        "          with torch.no_grad():\n",
        "              # spatial\n",
        "              pick = random.randint(0, 2)\n",
        "              spatial_input = videos[:,pick,:,:].permute(0,3,1,2)\n",
        "              spatial_output = spatial(spatial_input)\n",
        "\n",
        "              # temporal        \n",
        "              temporal_output = temporal(SG3I)\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # fused\n",
        "          fused_input = torch.hstack((spatial_output, temporal_output))\n",
        "          fused_output = fused(fused_input)\n",
        "\n",
        "          fused_output = fused_output.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # calculate losses\n",
        "          loss = fused.compute_loss(fused_output, labels)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          train_loss += loss.item()\n",
        "\n",
        "          # Logging Interval\n",
        "          log_int_loss += loss.item()\n",
        "          epoch_loss.append(loss.item())\n",
        "\n",
        "          if counter == 0:\n",
        "              print('Epoch: ', epoch, ', Batch: ', iter, ', loss avg over log interval: ', log_int_loss)\n",
        "              train_loss_list.append(train_loss / (iter + 1) * batch_size)\n",
        "              train_counter.append((iter + 1) * batch_size + epoch * 64)\n",
        "              log_int_loss = 0\n",
        "          elif counter % log_interval == log_interval - 1:\n",
        "              print('Epoch: ', epoch, ', Batch: ', iter, ', loss avg over log interval: ', log_int_loss / log_interval)\n",
        "              train_loss_list.append(train_loss / (iter + 1) * batch_size)\n",
        "              train_counter.append((iter + 1) * batch_size + epoch * 64)\n",
        "              log_int_loss = 0\n",
        "\n",
        "          # if counter % save_interval == save_interval - 1:\n",
        "          #     print('saving model')\n",
        "          #     save_path = os.path.join(EPOCH_SAVE_PREFIX, 'fused_sampled_epoch' + str(epoch) + '_iter_' + str(counter))\n",
        "          #     torch.save({\n",
        "          #         'epoch': epoch,\n",
        "          #         'train_total_loss_list': train_loss_list,\n",
        "          #         'epoch_total_loss_list': epoch_loss_list,\n",
        "          #         'test_loss_list': test_loss_list,\n",
        "          #         'train_counter': train_counter,\n",
        "          #         'accuracy_list': accuracy_list,\n",
        "          #         'model_state_dict': fused.state_dict(),\n",
        "          #         'optimizer_state_dict': optimizer.state_dict()\n",
        "          #     }, save_path)\n",
        "\n",
        "          counter += 1\n",
        "\n",
        "\n",
        "    avg_loss = sum(epoch_loss) / len(epoch_loss)\n",
        "    epoch_loss_list.append(avg_loss)\n",
        "    print('Epoch: ', epoch, ', avg total loss: ', avg_loss)\n",
        "\n",
        "def test():\n",
        "    fused.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    batch_iter = [0,64]\n",
        "    # Data Loop\n",
        "    with torch.no_grad():\n",
        "        for iter, data in enumerate(test_loader, 0):\n",
        "          for i in batch_iter:\n",
        "            videos = data[\"videos\"][0+i:i+64]\n",
        "            labels = torch.tensor(data[\"labels\"][0+i:i+64])\n",
        "            indexes = data[\"indexes\"][0+i:i+64]\n",
        "            \n",
        "            if len(videos) < 2:\n",
        "              break\n",
        "            SG3I = getSG3I(videos)\n",
        "            SG3I = SG3I.permute(0,3,1,2)\n",
        "            SG3I = SG3I.to(device)\n",
        "\n",
        "            videos = videos.type(torch.FloatTensor)\n",
        "            videos = videos.to(device)\n",
        "            # labels = labels.to(device)\n",
        "\n",
        "            # spatial\n",
        "            spatial_input = videos[:,0,:,:].permute(0,3,1,2)\n",
        "            spatial_output = spatial(spatial_input)\n",
        "\n",
        "            # temporal        \n",
        "            temporal_output = temporal(SG3I)\n",
        "            \n",
        "            # fused\n",
        "            fused_input = torch.hstack((spatial_output, temporal_output))\n",
        "            fused_output = fused(fused_input)\n",
        "\n",
        "            fused_output = fused_output.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # calculate losses\n",
        "            loss = fused.compute_loss(fused_output, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # calculate number of correct predictions in batch\n",
        "            correct += sum(torch.argmax(fused_output,1) == labels).item()\n",
        "            if iter % 100 == 0:\n",
        "                print (\"iter  \", iter)\n",
        "                print(\"accuracy so far = \", correct / ((iter + 1) * 128))\n",
        "\n",
        "    # Log\n",
        "    test_loss_list.append(test_loss / len(test_loader))\n",
        "    accuracy = correct / (5145)\n",
        "    accuracy_list.append(accuracy)\n",
        "    print('Avg Validation Loss: ', test_loss / len(test_loader))\n",
        "    print('Accuracy: ', accuracy)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxNfXCLLYcbW"
      },
      "source": [
        "# Main Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiW7i49DYbwc",
        "outputId": "968033aa-7fc4-44e1-a6db-911cb733dcf3"
      },
      "source": [
        "EPOCH_SAVE_PREFIX = '/content/drive/Shared drives/CIS680 Final Project/models/fused_sampled_3_relu_rahul_sgd/'\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "torch.random.manual_seed(1)\n",
        "\n",
        "# load trained spatial\n",
        "spatial = SpatialStream()\n",
        "spatial.to(device)\n",
        "spatial_network_path = '/content/drive/Shared drives/CIS680 Final Project/models/spatial_sampled_3/spatial_epoch445'\n",
        "checkpoint_spatial = torch.load(spatial_network_path)\n",
        "spatial.load_state_dict(checkpoint_spatial['model_state_dict'])\n",
        "\n",
        "# load trained temporal\n",
        "temporal = TemporalStream()\n",
        "temporal.to(device)\n",
        "temporal_network_path = '/content/drive/Shared drives/CIS680 Final Project/models/temporal_new_data/temporal_epoch288'\n",
        "checkpoint_temporal = torch.load(temporal_network_path)\n",
        "temporal.load_state_dict(checkpoint_temporal['model_state_dict'])\n",
        "\n",
        "\n",
        "# fused model\n",
        "learning_rate = 0.001\n",
        "fused = FuseNET()\n",
        "fused.to(device)\n",
        "optimizer = optim.AdamW(fused.parameters(), lr=learning_rate)#, momentum=0.9)\n",
        "\n",
        "# Epochs\n",
        "num_epochs = 250\n",
        "batch_size = 64\n",
        "\n",
        "# Logging setup: train\n",
        "train_loss_list = []\n",
        "epoch_loss_list = []\n",
        "train_counter = []\n",
        "\n",
        "# Logging setup: test\n",
        "test_loss_list = []\n",
        "accuracy_list = []\n",
        "epoch_list = np.arange(num_epochs)\n",
        "\n",
        "# epoch loop\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Train & Validate\n",
        "    train(epoch)\n",
        "    test()\n",
        "\n",
        "    # Save Model Version\n",
        "    save_path = os.path.join(EPOCH_SAVE_PREFIX, 'fused_sampled_epoch' + str(epoch))\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'train_total_loss_list': train_loss_list,\n",
        "        'epoch_total_loss_list': epoch_loss_list,\n",
        "        'test_loss_list': test_loss_list,\n",
        "        'train_counter': train_counter,\n",
        "        'accuracy_list': accuracy_list,\n",
        "        'model_state_dict': fused.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, save_path)\n",
        "\n",
        "    print(\"Epoch %d/%d Completed\" % (epoch, num_epochs - 1))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0 , Batch:  0 , loss avg over log interval:  3.931824207305908\n",
            "Epoch:  0 , Batch:  49 , loss avg over log interval:  3.889885129928589\n",
            "Epoch:  0 , Batch:  99 , loss avg over log interval:  3.922495193481445\n",
            "Epoch:  0 , avg total loss:  3.92252820039141\n",
            "iter   0\n",
            "accuracy so far =  0.4765625\n",
            "iter   0\n",
            "accuracy so far =  0.765625\n",
            "Avg Validation Loss:  7.7456827745205015\n",
            "Accuracy:  0.5172011661807581\n",
            "Epoch 0/249 Completed\n",
            "Epoch:  1 , Batch:  0 , loss avg over log interval:  3.906594753265381\n",
            "Epoch:  1 , Batch:  49 , loss avg over log interval:  3.8618234610557556\n",
            "Epoch:  1 , Batch:  99 , loss avg over log interval:  3.8784245920181273\n",
            "Epoch:  1 , avg total loss:  3.8777425980222398\n",
            "iter   0\n",
            "accuracy so far =  0.484375\n",
            "iter   0\n",
            "accuracy so far =  0.7734375\n",
            "Avg Validation Loss:  7.681601576688813\n",
            "Accuracy:  0.5327502429543246\n",
            "Epoch 1/249 Completed\n",
            "Epoch:  2 , Batch:  0 , loss avg over log interval:  3.8162405490875244\n",
            "Epoch:  2 , Batch:  49 , loss avg over log interval:  3.763819673061371\n",
            "Epoch:  2 , Batch:  99 , loss avg over log interval:  3.736292185783386\n",
            "Epoch:  2 , avg total loss:  3.7399322183235832\n",
            "iter   0\n",
            "accuracy so far =  0.484375\n",
            "iter   0\n",
            "accuracy so far =  0.78125\n",
            "Avg Validation Loss:  7.529305266170967\n",
            "Accuracy:  0.5407191448007774\n",
            "Epoch 2/249 Completed\n",
            "Epoch:  3 , Batch:  0 , loss avg over log interval:  3.5972118377685547\n",
            "Epoch:  3 , Batch:  49 , loss avg over log interval:  3.5603533363342286\n",
            "Epoch:  3 , Batch:  99 , loss avg over log interval:  3.5263190174102785\n",
            "Epoch:  3 , avg total loss:  3.5340937747471575\n",
            "iter   0\n",
            "accuracy so far =  0.4765625\n",
            "iter   0\n",
            "accuracy so far =  0.7734375\n",
            "Avg Validation Loss:  7.359670883271752\n",
            "Accuracy:  0.545578231292517\n",
            "Epoch 3/249 Completed\n",
            "Epoch:  4 , Batch:  0 , loss avg over log interval:  3.3892593383789062\n",
            "Epoch:  4 , Batch:  49 , loss avg over log interval:  3.383534014225006\n",
            "Epoch:  4 , Batch:  99 , loss avg over log interval:  3.3754393339157103\n",
            "Epoch:  4 , avg total loss:  3.379159200882566\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.765625\n",
            "Avg Validation Loss:  7.238371662977265\n",
            "Accuracy:  0.5504373177842565\n",
            "Epoch 4/249 Completed\n",
            "Epoch:  5 , Batch:  0 , loss avg over log interval:  3.257988691329956\n",
            "Epoch:  5 , Batch:  49 , loss avg over log interval:  3.275315029621124\n",
            "Epoch:  5 , Batch:  99 , loss avg over log interval:  3.2840876746177674\n",
            "Epoch:  5 , avg total loss:  3.2854871827623118\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.7734375\n",
            "Avg Validation Loss:  7.156906761774203\n",
            "Accuracy:  0.5521865889212828\n",
            "Epoch 5/249 Completed\n",
            "Epoch:  6 , Batch:  0 , loss avg over log interval:  3.1833369731903076\n",
            "Epoch:  6 , Batch:  49 , loss avg over log interval:  3.2090674448013305\n",
            "Epoch:  6 , Batch:  99 , loss avg over log interval:  3.227495963573456\n",
            "Epoch:  6 , avg total loss:  3.22662514620933\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.75\n",
            "Avg Validation Loss:  7.100085938849101\n",
            "Accuracy:  0.5510204081632653\n",
            "Epoch 6/249 Completed\n",
            "Epoch:  7 , Batch:  0 , loss avg over log interval:  3.13331937789917\n",
            "Epoch:  7 , Batch:  49 , loss avg over log interval:  3.16541405916214\n",
            "Epoch:  7 , Batch:  99 , loss avg over log interval:  3.189582769870758\n",
            "Epoch:  7 , avg total loss:  3.1879754645237024\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.75\n",
            "Avg Validation Loss:  7.058674794871632\n",
            "Accuracy:  0.5525753158406219\n",
            "Epoch 7/249 Completed\n",
            "Epoch:  8 , Batch:  0 , loss avg over log interval:  3.102860450744629\n",
            "Epoch:  8 , Batch:  49 , loss avg over log interval:  3.1369612336158754\n",
            "Epoch:  8 , Batch:  99 , loss avg over log interval:  3.1636679553985596\n",
            "Epoch:  8 , avg total loss:  3.161359534747359\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.75\n",
            "Avg Validation Loss:  7.027243387408372\n",
            "Accuracy:  0.5529640427599611\n",
            "Epoch 8/249 Completed\n",
            "Epoch:  9 , Batch:  0 , loss avg over log interval:  3.079836368560791\n",
            "Epoch:  9 , Batch:  49 , loss avg over log interval:  3.1144463801383973\n",
            "Epoch:  9 , Batch:  99 , loss avg over log interval:  3.143999536037445\n",
            "Epoch:  9 , avg total loss:  3.1414922588113425\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.7421875\n",
            "Avg Validation Loss:  7.002331733703613\n",
            "Accuracy:  0.5533527696793002\n",
            "Epoch 9/249 Completed\n",
            "Epoch:  10 , Batch:  0 , loss avg over log interval:  3.0653834342956543\n",
            "Epoch:  10 , Batch:  49 , loss avg over log interval:  3.097235794067383\n",
            "Epoch:  10 , Batch:  99 , loss avg over log interval:  3.129007513523102\n",
            "Epoch:  10 , avg total loss:  3.126502383446348\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.7421875\n",
            "Avg Validation Loss:  6.98190555921415\n",
            "Accuracy:  0.5529640427599611\n",
            "Epoch 10/249 Completed\n",
            "Epoch:  11 , Batch:  0 , loss avg over log interval:  3.0574584007263184\n",
            "Epoch:  11 , Batch:  49 , loss avg over log interval:  3.0865334725379943\n",
            "Epoch:  11 , Batch:  99 , loss avg over log interval:  3.1200259518623352\n",
            "Epoch:  11 , avg total loss:  3.1166035014650095\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.7421875\n",
            "Avg Validation Loss:  6.964863416625232\n",
            "Accuracy:  0.5533527696793002\n",
            "Epoch 11/249 Completed\n",
            "Epoch:  12 , Batch:  0 , loss avg over log interval:  3.0539960861206055\n",
            "Epoch:  12 , Batch:  49 , loss avg over log interval:  3.076010663509369\n",
            "Epoch:  12 , Batch:  99 , loss avg over log interval:  3.1114037561416628\n",
            "Epoch:  12 , avg total loss:  3.107017484264097\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.7421875\n",
            "Avg Validation Loss:  6.950353791074055\n",
            "Accuracy:  0.5531584062196307\n",
            "Epoch 12/249 Completed\n",
            "Epoch:  13 , Batch:  0 , loss avg over log interval:  3.0474600791931152\n",
            "Epoch:  13 , Batch:  49 , loss avg over log interval:  3.069569764137268\n",
            "Epoch:  13 , Batch:  99 , loss avg over log interval:  3.10265695810318\n",
            "Epoch:  13 , avg total loss:  3.099467501260232\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.734375\n",
            "Avg Validation Loss:  6.937786567501906\n",
            "Accuracy:  0.5531584062196307\n",
            "Epoch 13/249 Completed\n",
            "Epoch:  14 , Batch:  0 , loss avg over log interval:  3.0419740676879883\n",
            "Epoch:  14 , Batch:  49 , loss avg over log interval:  3.062363474369049\n",
            "Epoch:  14 , Batch:  99 , loss avg over log interval:  3.094841754436493\n",
            "Epoch:  14 , avg total loss:  3.0922652379326196\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.734375\n",
            "Avg Validation Loss:  6.927014321815677\n",
            "Accuracy:  0.5533527696793002\n",
            "Epoch 14/249 Completed\n",
            "Epoch:  15 , Batch:  0 , loss avg over log interval:  3.037287712097168\n",
            "Epoch:  15 , Batch:  49 , loss avg over log interval:  3.05511479139328\n",
            "Epoch:  15 , Batch:  99 , loss avg over log interval:  3.089275548458099\n",
            "Epoch:  15 , avg total loss:  3.08637944943663\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.7265625\n",
            "Avg Validation Loss:  6.917419259141131\n",
            "Accuracy:  0.5527696793002915\n",
            "Epoch 15/249 Completed\n",
            "Epoch:  16 , Batch:  0 , loss avg over log interval:  3.025923728942871\n",
            "Epoch:  16 , Batch:  49 , loss avg over log interval:  3.0522793316841126\n",
            "Epoch:  16 , Batch:  99 , loss avg over log interval:  3.085991613864899\n",
            "Epoch:  16 , avg total loss:  3.0826101035311604\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.7265625\n",
            "Avg Validation Loss:  6.909063740474422\n",
            "Accuracy:  0.5521865889212828\n",
            "Epoch 16/249 Completed\n",
            "Epoch:  17 , Batch:  0 , loss avg over log interval:  3.029907703399658\n",
            "Epoch:  17 , Batch:  49 , loss avg over log interval:  3.046306495666504\n",
            "Epoch:  17 , Batch:  99 , loss avg over log interval:  3.0827289700508116\n",
            "Epoch:  17 , avg total loss:  3.0782054317170293\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.7265625\n",
            "Avg Validation Loss:  6.901512593757816\n",
            "Accuracy:  0.5514091350826045\n",
            "Epoch 17/249 Completed\n",
            "Epoch:  18 , Batch:  0 , loss avg over log interval:  3.022472381591797\n",
            "Epoch:  18 , Batch:  49 , loss avg over log interval:  3.042259786128998\n",
            "Epoch:  18 , Batch:  99 , loss avg over log interval:  3.079134886264801\n",
            "Epoch:  18 , avg total loss:  3.0741814167603203\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.7265625\n",
            "Avg Validation Loss:  6.8949822623555255\n",
            "Accuracy:  0.5494655004859087\n",
            "Epoch 18/249 Completed\n",
            "Epoch:  19 , Batch:  0 , loss avg over log interval:  3.0200016498565674\n",
            "Epoch:  19 , Batch:  49 , loss avg over log interval:  3.0387178754806516\n",
            "Epoch:  19 , Batch:  99 , loss avg over log interval:  3.0756661438941957\n",
            "Epoch:  19 , avg total loss:  3.0714478734610737\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.7265625\n",
            "Avg Validation Loss:  6.8891943605934705\n",
            "Accuracy:  0.5486880466472304\n",
            "Epoch 19/249 Completed\n",
            "Epoch:  20 , Batch:  0 , loss avg over log interval:  3.015660285949707\n",
            "Epoch:  20 , Batch:  49 , loss avg over log interval:  3.037468252182007\n",
            "Epoch:  20 , Batch:  99 , loss avg over log interval:  3.0713936376571653\n",
            "Epoch:  20 , avg total loss:  3.068707393563312\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.71875\n",
            "Avg Validation Loss:  6.883862594278847\n",
            "Accuracy:  0.5484936831875608\n",
            "Epoch 20/249 Completed\n",
            "Epoch:  21 , Batch:  0 , loss avg over log interval:  3.0201690196990967\n",
            "Epoch:  21 , Batch:  49 , loss avg over log interval:  3.033317172527313\n",
            "Epoch:  21 , Batch:  99 , loss avg over log interval:  3.070457122325897\n",
            "Epoch:  21 , avg total loss:  3.066139672977337\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.71875\n",
            "Avg Validation Loss:  6.8794304394140475\n",
            "Accuracy:  0.5471331389698737\n",
            "Epoch 21/249 Completed\n",
            "Epoch:  22 , Batch:  0 , loss avg over log interval:  3.018296718597412\n",
            "Epoch:  22 , Batch:  49 , loss avg over log interval:  3.031863260269165\n",
            "Epoch:  22 , Batch:  99 , loss avg over log interval:  3.068237361907959\n",
            "Epoch:  22 , avg total loss:  3.06428318179172\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.71875\n",
            "Avg Validation Loss:  6.875137515184356\n",
            "Accuracy:  0.5461613216715258\n",
            "Epoch 22/249 Completed\n",
            "Epoch:  23 , Batch:  0 , loss avg over log interval:  3.016874313354492\n",
            "Epoch:  23 , Batch:  49 , loss avg over log interval:  3.0305146884918215\n",
            "Epoch:  23 , Batch:  99 , loss avg over log interval:  3.065300393104553\n",
            "Epoch:  23 , avg total loss:  3.0620633398277173\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.71875\n",
            "Avg Validation Loss:  6.87163687333828\n",
            "Accuracy:  0.545578231292517\n",
            "Epoch 23/249 Completed\n",
            "Epoch:  24 , Batch:  0 , loss avg over log interval:  3.011970281600952\n",
            "Epoch:  24 , Batch:  49 , loss avg over log interval:  3.0268117952346802\n",
            "Epoch:  24 , Batch:  99 , loss avg over log interval:  3.0640725970268248\n",
            "Epoch:  24 , avg total loss:  3.0596956170123555\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.71875\n",
            "Avg Validation Loss:  6.86845051370016\n",
            "Accuracy:  0.5449951409135083\n",
            "Epoch 24/249 Completed\n",
            "Epoch:  25 , Batch:  0 , loss avg over log interval:  3.0091559886932373\n",
            "Epoch:  25 , Batch:  49 , loss avg over log interval:  3.027126879692078\n",
            "Epoch:  25 , Batch:  99 , loss avg over log interval:  3.0606551361083985\n",
            "Epoch:  25 , avg total loss:  3.0579179102096004\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.71875\n",
            "Avg Validation Loss:  6.865601132555706\n",
            "Accuracy:  0.54421768707483\n",
            "Epoch 25/249 Completed\n",
            "Epoch:  26 , Batch:  0 , loss avg over log interval:  3.008274793624878\n",
            "Epoch:  26 , Batch:  49 , loss avg over log interval:  3.0246178030967714\n",
            "Epoch:  26 , Batch:  99 , loss avg over log interval:  3.060819857120514\n",
            "Epoch:  26 , avg total loss:  3.0567369288292485\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.71875\n",
            "Avg Validation Loss:  6.863279656666081\n",
            "Accuracy:  0.5428571428571428\n",
            "Epoch 26/249 Completed\n",
            "Epoch:  27 , Batch:  0 , loss avg over log interval:  3.007629871368408\n",
            "Epoch:  27 , Batch:  49 , loss avg over log interval:  3.023121919631958\n",
            "Epoch:  27 , Batch:  99 , loss avg over log interval:  3.0598273038864137\n",
            "Epoch:  27 , avg total loss:  3.055992279363715\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.71875\n",
            "Avg Validation Loss:  6.861069481547286\n",
            "Accuracy:  0.541885325558795\n",
            "Epoch 27/249 Completed\n",
            "Epoch:  28 , Batch:  0 , loss avg over log interval:  3.0070555210113525\n",
            "Epoch:  28 , Batch:  49 , loss avg over log interval:  3.022191412448883\n",
            "Epoch:  28 , Batch:  99 , loss avg over log interval:  3.059430646896362\n",
            "Epoch:  28 , avg total loss:  3.0545761611150657\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.71875\n",
            "Avg Validation Loss:  6.859001502758119\n",
            "Accuracy:  0.540913508260447\n",
            "Epoch 28/249 Completed\n",
            "Epoch:  29 , Batch:  0 , loss avg over log interval:  3.0082342624664307\n",
            "Epoch:  29 , Batch:  49 , loss avg over log interval:  3.0218629264831542\n",
            "Epoch:  29 , Batch:  99 , loss avg over log interval:  3.057417645454407\n",
            "Epoch:  29 , avg total loss:  3.0535076908443286\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.71875\n",
            "Avg Validation Loss:  6.857211333949391\n",
            "Accuracy:  0.5399416909620991\n",
            "Epoch 29/249 Completed\n",
            "Epoch:  30 , Batch:  0 , loss avg over log interval:  3.0062100887298584\n",
            "Epoch:  30 , Batch:  49 , loss avg over log interval:  3.0198510694503784\n",
            "Epoch:  30 , Batch:  99 , loss avg over log interval:  3.0558223509788514\n",
            "Epoch:  30 , avg total loss:  3.05190863903018\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.71875\n",
            "Avg Validation Loss:  6.855667242189733\n",
            "Accuracy:  0.53955296404276\n",
            "Epoch 30/249 Completed\n",
            "Epoch:  31 , Batch:  0 , loss avg over log interval:  3.0058395862579346\n",
            "Epoch:  31 , Batch:  49 , loss avg over log interval:  3.018715796470642\n",
            "Epoch:  31 , Batch:  99 , loss avg over log interval:  3.0549075865745543\n",
            "Epoch:  31 , avg total loss:  3.050844062929568\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.71875\n",
            "Avg Validation Loss:  6.854451115538434\n",
            "Accuracy:  0.5389698736637513\n",
            "Epoch 31/249 Completed\n",
            "Epoch:  32 , Batch:  0 , loss avg over log interval:  3.0055370330810547\n",
            "Epoch:  32 , Batch:  49 , loss avg over log interval:  3.0181688833236695\n",
            "Epoch:  32 , Batch:  99 , loss avg over log interval:  3.0542607831954958\n",
            "Epoch:  32 , avg total loss:  3.0500028936759285\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.7109375\n",
            "Avg Validation Loss:  6.853406580483041\n",
            "Accuracy:  0.536248785228377\n",
            "Epoch 32/249 Completed\n",
            "Epoch:  33 , Batch:  0 , loss avg over log interval:  3.010026454925537\n",
            "Epoch:  33 , Batch:  49 , loss avg over log interval:  3.017649188041687\n",
            "Epoch:  33 , Batch:  99 , loss avg over log interval:  3.0541078639030457\n",
            "Epoch:  33 , avg total loss:  3.050056152585624\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.703125\n",
            "Avg Validation Loss:  6.852422737493748\n",
            "Accuracy:  0.5354713313896987\n",
            "Epoch 33/249 Completed\n",
            "Epoch:  34 , Batch:  0 , loss avg over log interval:  3.0096683502197266\n",
            "Epoch:  34 , Batch:  49 , loss avg over log interval:  3.016374795436859\n",
            "Epoch:  34 , Batch:  99 , loss avg over log interval:  3.0519274854660035\n",
            "Epoch:  34 , avg total loss:  3.0486934997033384\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.703125\n",
            "Avg Validation Loss:  6.851447146113326\n",
            "Accuracy:  0.5358600583090379\n",
            "Epoch 34/249 Completed\n",
            "Epoch:  35 , Batch:  0 , loss avg over log interval:  3.004882574081421\n",
            "Epoch:  35 , Batch:  49 , loss avg over log interval:  3.0163262343406676\n",
            "Epoch:  35 , Batch:  99 , loss avg over log interval:  3.0525040102005003\n",
            "Epoch:  35 , avg total loss:  3.048378578994585\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.6953125\n",
            "Avg Validation Loss:  6.850822815080968\n",
            "Accuracy:  0.5346938775510204\n",
            "Epoch 35/249 Completed\n",
            "Epoch:  36 , Batch:  0 , loss avg over log interval:  3.009280204772949\n",
            "Epoch:  36 , Batch:  49 , loss avg over log interval:  3.0144503045082094\n",
            "Epoch:  36 , Batch:  99 , loss avg over log interval:  3.051981899738312\n",
            "Epoch:  36 , avg total loss:  3.0475191033404805\n",
            "iter   0\n",
            "accuracy so far =  0.46875\n",
            "iter   0\n",
            "accuracy so far =  0.6953125\n",
            "Avg Validation Loss:  6.849943207531441\n",
            "Accuracy:  0.5337220602526725\n",
            "Epoch 36/249 Completed\n",
            "Epoch:  37 , Batch:  0 , loss avg over log interval:  3.005984306335449\n",
            "Epoch:  37 , Batch:  49 , loss avg over log interval:  3.0142955565452576\n",
            "Epoch:  37 , Batch:  99 , loss avg over log interval:  3.0507742094993593\n",
            "Epoch:  37 , avg total loss:  3.046524040940879\n",
            "iter   0\n",
            "accuracy so far =  0.4609375\n",
            "iter   0\n",
            "accuracy so far =  0.6875\n",
            "Avg Validation Loss:  6.849599059035138\n",
            "Accuracy:  0.5319727891156463\n",
            "Epoch 37/249 Completed\n",
            "Epoch:  38 , Batch:  0 , loss avg over log interval:  3.0087954998016357\n",
            "Epoch:  38 , Batch:  49 , loss avg over log interval:  3.014832730293274\n",
            "Epoch:  38 , Batch:  99 , loss avg over log interval:  3.0502832746505737\n",
            "Epoch:  38 , avg total loss:  3.046522633752961\n",
            "iter   0\n",
            "accuracy so far =  0.4609375\n",
            "iter   0\n",
            "accuracy so far =  0.6875\n",
            "Avg Validation Loss:  6.849135957113126\n",
            "Accuracy:  0.5321671525753159\n",
            "Epoch 38/249 Completed\n",
            "Epoch:  39 , Batch:  0 , loss avg over log interval:  3.0043344497680664\n",
            "Epoch:  39 , Batch:  49 , loss avg over log interval:  3.014588944911957\n",
            "Epoch:  39 , Batch:  99 , loss avg over log interval:  3.04936279296875\n",
            "Epoch:  39 , avg total loss:  3.0459662839986277\n",
            "iter   0\n",
            "accuracy so far =  0.4609375\n",
            "iter   0\n",
            "accuracy so far =  0.6875\n",
            "Avg Validation Loss:  6.8485140974928695\n",
            "Accuracy:  0.5327502429543246\n",
            "Epoch 39/249 Completed\n",
            "Epoch:  40 , Batch:  0 , loss avg over log interval:  3.005547523498535\n",
            "Epoch:  40 , Batch:  49 , loss avg over log interval:  3.012494740486145\n",
            "Epoch:  40 , Batch:  99 , loss avg over log interval:  3.050059220790863\n",
            "Epoch:  40 , avg total loss:  3.044996069825214\n",
            "iter   0\n",
            "accuracy so far =  0.4609375\n",
            "iter   0\n",
            "accuracy so far =  0.6875\n",
            "Avg Validation Loss:  6.8482218486506765\n",
            "Accuracy:  0.5323615160349854\n",
            "Epoch 40/249 Completed\n",
            "Epoch:  41 , Batch:  0 , loss avg over log interval:  3.0041027069091797\n",
            "Epoch:  41 , Batch:  49 , loss avg over log interval:  3.0130537509918214\n",
            "Epoch:  41 , Batch:  99 , loss avg over log interval:  3.0487237477302553\n",
            "Epoch:  41 , avg total loss:  3.045036274453868\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6640625\n",
            "Avg Validation Loss:  6.848131598495856\n",
            "Accuracy:  0.5321671525753159\n",
            "Epoch 41/249 Completed\n",
            "Epoch:  42 , Batch:  0 , loss avg over log interval:  3.0053353309631348\n",
            "Epoch:  42 , Batch:  49 , loss avg over log interval:  3.0130426049232484\n",
            "Epoch:  42 , Batch:  99 , loss avg over log interval:  3.0487970900535584\n",
            "Epoch:  42 , avg total loss:  3.0448399073835732\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6640625\n",
            "Avg Validation Loss:  6.847972491892373\n",
            "Accuracy:  0.5319727891156463\n",
            "Epoch 42/249 Completed\n",
            "Epoch:  43 , Batch:  0 , loss avg over log interval:  3.0052788257598877\n",
            "Epoch:  43 , Batch:  49 , loss avg over log interval:  3.0127795791625975\n",
            "Epoch:  43 , Batch:  99 , loss avg over log interval:  3.0480484175682068\n",
            "Epoch:  43 , avg total loss:  3.0441279428592627\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6640625\n",
            "Avg Validation Loss:  6.847650469803229\n",
            "Accuracy:  0.532555879494655\n",
            "Epoch 43/249 Completed\n",
            "Epoch:  44 , Batch:  0 , loss avg over log interval:  3.0082733631134033\n",
            "Epoch:  44 , Batch:  49 , loss avg over log interval:  3.009896197319031\n",
            "Epoch:  44 , Batch:  99 , loss avg over log interval:  3.0472229647636415\n",
            "Epoch:  44 , avg total loss:  3.0424451171488003\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.65625\n",
            "Avg Validation Loss:  6.847587149317672\n",
            "Accuracy:  0.5323615160349854\n",
            "Epoch 44/249 Completed\n",
            "Epoch:  45 , Batch:  0 , loss avg over log interval:  3.0039069652557373\n",
            "Epoch:  45 , Batch:  49 , loss avg over log interval:  3.0114949774742126\n",
            "Epoch:  45 , Batch:  99 , loss avg over log interval:  3.04717679977417\n",
            "Epoch:  45 , avg total loss:  3.0435431314551313\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.65625\n",
            "Avg Validation Loss:  6.847333390538285\n",
            "Accuracy:  0.5311953352769679\n",
            "Epoch 45/249 Completed\n",
            "Epoch:  46 , Batch:  0 , loss avg over log interval:  3.003857374191284\n",
            "Epoch:  46 , Batch:  49 , loss avg over log interval:  3.0106852579116823\n",
            "Epoch:  46 , Batch:  99 , loss avg over log interval:  3.0455477809906006\n",
            "Epoch:  46 , avg total loss:  3.0421466084494106\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.846800024916486\n",
            "Accuracy:  0.5310009718172983\n",
            "Epoch 46/249 Completed\n",
            "Epoch:  47 , Batch:  0 , loss avg over log interval:  3.0050320625305176\n",
            "Epoch:  47 , Batch:  49 , loss avg over log interval:  3.010864315032959\n",
            "Epoch:  47 , Batch:  99 , loss avg over log interval:  3.046428391933441\n",
            "Epoch:  47 , avg total loss:  3.0425645095714624\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.846732790877179\n",
            "Accuracy:  0.5306122448979592\n",
            "Epoch 47/249 Completed\n",
            "Epoch:  48 , Batch:  0 , loss avg over log interval:  3.0049707889556885\n",
            "Epoch:  48 , Batch:  49 , loss avg over log interval:  3.008995945453644\n",
            "Epoch:  48 , Batch:  99 , loss avg over log interval:  3.046002652645111\n",
            "Epoch:  48 , avg total loss:  3.0417514426120813\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.846847365542156\n",
            "Accuracy:  0.5290573372206026\n",
            "Epoch 48/249 Completed\n",
            "Epoch:  49 , Batch:  0 , loss avg over log interval:  3.003634214401245\n",
            "Epoch:  49 , Batch:  49 , loss avg over log interval:  3.009760262966156\n",
            "Epoch:  49 , Batch:  99 , loss avg over log interval:  3.0450209093093874\n",
            "Epoch:  49 , avg total loss:  3.041493241337762\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.846421672076714\n",
            "Accuracy:  0.5280855199222546\n",
            "Epoch 49/249 Completed\n",
            "Epoch:  50 , Batch:  0 , loss avg over log interval:  3.0049211978912354\n",
            "Epoch:  50 , Batch:  49 , loss avg over log interval:  3.0092487215995787\n",
            "Epoch:  50 , Batch:  99 , loss avg over log interval:  3.0448002648353576\n",
            "Epoch:  50 , avg total loss:  3.0410886424175207\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.846315255979213\n",
            "Accuracy:  0.5282798833819242\n",
            "Epoch 50/249 Completed\n",
            "Epoch:  51 , Batch:  0 , loss avg over log interval:  3.004878520965576\n",
            "Epoch:  51 , Batch:  49 , loss avg over log interval:  3.0097556638717653\n",
            "Epoch:  51 , Batch:  99 , loss avg over log interval:  3.0443672823905943\n",
            "Epoch:  51 , avg total loss:  3.040642619996831\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.846229326434251\n",
            "Accuracy:  0.5286686103012633\n",
            "Epoch 51/249 Completed\n",
            "Epoch:  52 , Batch:  0 , loss avg over log interval:  3.0035481452941895\n",
            "Epoch:  52 , Batch:  49 , loss avg over log interval:  3.0080709719657897\n",
            "Epoch:  52 , Batch:  99 , loss avg over log interval:  3.0445367002487185\n",
            "Epoch:  52 , avg total loss:  3.040977556636368\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845852311064557\n",
            "Accuracy:  0.5276967930029155\n",
            "Epoch 52/249 Completed\n",
            "Epoch:  53 , Batch:  0 , loss avg over log interval:  3.0034854412078857\n",
            "Epoch:  53 , Batch:  49 , loss avg over log interval:  3.007222912311554\n",
            "Epoch:  53 , Batch:  99 , loss avg over log interval:  3.0435610818862915\n",
            "Epoch:  53 , avg total loss:  3.0398022731145224\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845535801678169\n",
            "Accuracy:  0.5280855199222546\n",
            "Epoch 53/249 Completed\n",
            "Epoch:  54 , Batch:  0 , loss avg over log interval:  3.0046961307525635\n",
            "Epoch:  54 , Batch:  49 , loss avg over log interval:  3.0082754826545717\n",
            "Epoch:  54 , Batch:  99 , loss avg over log interval:  3.043147258758545\n",
            "Epoch:  54 , avg total loss:  3.0399324470672053\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845349945673129\n",
            "Accuracy:  0.5275024295432459\n",
            "Epoch 54/249 Completed\n",
            "Epoch:  55 , Batch:  0 , loss avg over log interval:  3.003361940383911\n",
            "Epoch:  55 , Batch:  49 , loss avg over log interval:  3.0091335487365725\n",
            "Epoch:  55 , Batch:  99 , loss avg over log interval:  3.044167001247406\n",
            "Epoch:  55 , avg total loss:  3.0404277962187063\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845485867523566\n",
            "Accuracy:  0.5275024295432459\n",
            "Epoch 55/249 Completed\n",
            "Epoch:  56 , Batch:  0 , loss avg over log interval:  3.0079596042633057\n",
            "Epoch:  56 , Batch:  49 , loss avg over log interval:  3.0071625256538392\n",
            "Epoch:  56 , Batch:  99 , loss avg over log interval:  3.042177436351776\n",
            "Epoch:  56 , avg total loss:  3.0388717158980993\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845506720426606\n",
            "Accuracy:  0.5271137026239067\n",
            "Epoch 56/249 Completed\n",
            "Epoch:  57 , Batch:  0 , loss avg over log interval:  3.0078518390655518\n",
            "Epoch:  57 , Batch:  49 , loss avg over log interval:  3.008125355243683\n",
            "Epoch:  57 , Batch:  99 , loss avg over log interval:  3.0443422055244445\n",
            "Epoch:  57 , avg total loss:  3.0401967262876206\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845665030363129\n",
            "Accuracy:  0.5269193391642372\n",
            "Epoch 57/249 Completed\n",
            "Epoch:  58 , Batch:  0 , loss avg over log interval:  3.0032832622528076\n",
            "Epoch:  58 , Batch:  49 , loss avg over log interval:  3.0072135400772093\n",
            "Epoch:  58 , Batch:  99 , loss avg over log interval:  3.0427058339118958\n",
            "Epoch:  58 , avg total loss:  3.039090769014497\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845628476724392\n",
            "Accuracy:  0.5259475218658892\n",
            "Epoch 58/249 Completed\n",
            "Epoch:  59 , Batch:  0 , loss avg over log interval:  3.0079081058502197\n",
            "Epoch:  59 , Batch:  49 , loss avg over log interval:  3.007811107635498\n",
            "Epoch:  59 , Batch:  99 , loss avg over log interval:  3.0421084141731263\n",
            "Epoch:  59 , avg total loss:  3.03914022013761\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.8455015857045245\n",
            "Accuracy:  0.5259475218658892\n",
            "Epoch 59/249 Completed\n",
            "Epoch:  60 , Batch:  0 , loss avg over log interval:  3.0031280517578125\n",
            "Epoch:  60 , Batch:  49 , loss avg over log interval:  3.0078945350646973\n",
            "Epoch:  60 , Batch:  99 , loss avg over log interval:  3.042480010986328\n",
            "Epoch:  60 , avg total loss:  3.038932117862978\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845456291989582\n",
            "Accuracy:  0.5257531584062196\n",
            "Epoch 60/249 Completed\n",
            "Epoch:  61 , Batch:  0 , loss avg over log interval:  3.0043997764587402\n",
            "Epoch:  61 , Batch:  49 , loss avg over log interval:  3.006819586753845\n",
            "Epoch:  61 , Batch:  99 , loss avg over log interval:  3.040328035354614\n",
            "Epoch:  61 , avg total loss:  3.0378847372704656\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845303837845965\n",
            "Accuracy:  0.5265306122448979\n",
            "Epoch 61/249 Completed\n",
            "Epoch:  62 , Batch:  0 , loss avg over log interval:  3.0043492317199707\n",
            "Epoch:  62 , Batch:  49 , loss avg over log interval:  3.0067879581451415\n",
            "Epoch:  62 , Batch:  99 , loss avg over log interval:  3.041951138973236\n",
            "Epoch:  62 , avg total loss:  3.0384498774141506\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845392750530708\n",
            "Accuracy:  0.5259475218658892\n",
            "Epoch 62/249 Completed\n",
            "Epoch:  63 , Batch:  0 , loss avg over log interval:  3.0029594898223877\n",
            "Epoch:  63 , Batch:  49 , loss avg over log interval:  3.007680013179779\n",
            "Epoch:  63 , Batch:  99 , loss avg over log interval:  3.040824327468872\n",
            "Epoch:  63 , avg total loss:  3.0381734673527703\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.84528944550491\n",
            "Accuracy:  0.5257531584062196\n",
            "Epoch 63/249 Completed\n",
            "Epoch:  64 , Batch:  0 , loss avg over log interval:  3.007918357849121\n",
            "Epoch:  64 , Batch:  49 , loss avg over log interval:  3.0057806086540224\n",
            "Epoch:  64 , Batch:  99 , loss avg over log interval:  3.0433777379989624\n",
            "Epoch:  64 , avg total loss:  3.0384093289789944\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845251845150459\n",
            "Accuracy:  0.5265306122448979\n",
            "Epoch 64/249 Completed\n",
            "Epoch:  65 , Batch:  0 , loss avg over log interval:  3.0042338371276855\n",
            "Epoch:  65 , Batch:  49 , loss avg over log interval:  3.00652544260025\n",
            "Epoch:  65 , Batch:  99 , loss avg over log interval:  3.0418074774742125\n",
            "Epoch:  65 , avg total loss:  3.037894074467645\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845229939716618\n",
            "Accuracy:  0.5263362487852283\n",
            "Epoch 65/249 Completed\n",
            "Epoch:  66 , Batch:  0 , loss avg over log interval:  3.0042426586151123\n",
            "Epoch:  66 , Batch:  49 , loss avg over log interval:  3.00754940032959\n",
            "Epoch:  66 , Batch:  99 , loss avg over log interval:  3.0414318323135374\n",
            "Epoch:  66 , avg total loss:  3.038457474846771\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845268301847504\n",
            "Accuracy:  0.52555879494655\n",
            "Epoch 66/249 Completed\n",
            "Epoch:  67 , Batch:  0 , loss avg over log interval:  3.004225492477417\n",
            "Epoch:  67 , Batch:  49 , loss avg over log interval:  3.0067764067649843\n",
            "Epoch:  67 , Batch:  99 , loss avg over log interval:  3.0408883118629455\n",
            "Epoch:  67 , avg total loss:  3.0379292083823164\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845609804479087\n",
            "Accuracy:  0.52555879494655\n",
            "Epoch 67/249 Completed\n",
            "Epoch:  68 , Batch:  0 , loss avg over log interval:  3.0028648376464844\n",
            "Epoch:  68 , Batch:  49 , loss avg over log interval:  3.0052205443382265\n",
            "Epoch:  68 , Batch:  99 , loss avg over log interval:  3.040222532749176\n",
            "Epoch:  68 , avg total loss:  3.0363880084908526\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845400286883843\n",
            "Accuracy:  0.52555879494655\n",
            "Epoch 68/249 Completed\n",
            "Epoch:  69 , Batch:  0 , loss avg over log interval:  3.002767324447632\n",
            "Epoch:  69 , Batch:  49 , loss avg over log interval:  3.0047874903678893\n",
            "Epoch:  69 , Batch:  99 , loss avg over log interval:  3.0410187005996705\n",
            "Epoch:  69 , avg total loss:  3.0370317300160727\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845507627580224\n",
            "Accuracy:  0.5253644314868805\n",
            "Epoch 69/249 Completed\n",
            "Epoch:  70 , Batch:  0 , loss avg over log interval:  3.008115530014038\n",
            "Epoch:  70 , Batch:  49 , loss avg over log interval:  3.0057243371009825\n",
            "Epoch:  70 , Batch:  99 , loss avg over log interval:  3.0408090496063234\n",
            "Epoch:  70 , avg total loss:  3.0374091535374736\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845383911598019\n",
            "Accuracy:  0.5251700680272109\n",
            "Epoch 70/249 Completed\n",
            "Epoch:  71 , Batch:  0 , loss avg over log interval:  3.00266432762146\n",
            "Epoch:  71 , Batch:  49 , loss avg over log interval:  3.003709888458252\n",
            "Epoch:  71 , Batch:  99 , loss avg over log interval:  3.0409404540061953\n",
            "Epoch:  71 , avg total loss:  3.0364360731580984\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.8453728222265475\n",
            "Accuracy:  0.5245869776482022\n",
            "Epoch 71/249 Completed\n",
            "Epoch:  72 , Batch:  0 , loss avg over log interval:  3.0026142597198486\n",
            "Epoch:  72 , Batch:  49 , loss avg over log interval:  3.006289391517639\n",
            "Epoch:  72 , Batch:  99 , loss avg over log interval:  3.0403124046325685\n",
            "Epoch:  72 , avg total loss:  3.0376291801964026\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.844866351383488\n",
            "Accuracy:  0.5249757045675413\n",
            "Epoch 72/249 Completed\n",
            "Epoch:  73 , Batch:  0 , loss avg over log interval:  3.0025391578674316\n",
            "Epoch:  73 , Batch:  49 , loss avg over log interval:  3.0037753129005433\n",
            "Epoch:  73 , Batch:  99 , loss avg over log interval:  3.039303481578827\n",
            "Epoch:  73 , avg total loss:  3.035620237606159\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.844940197177049\n",
            "Accuracy:  0.5247813411078717\n",
            "Epoch 73/249 Completed\n",
            "Epoch:  74 , Batch:  0 , loss avg over log interval:  3.002455472946167\n",
            "Epoch:  74 , Batch:  49 , loss avg over log interval:  3.0058387088775635\n",
            "Epoch:  74 , Batch:  99 , loss avg over log interval:  3.038961625099182\n",
            "Epoch:  74 , avg total loss:  3.036137453023938\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845005750656128\n",
            "Accuracy:  0.52555879494655\n",
            "Epoch 74/249 Completed\n",
            "Epoch:  75 , Batch:  0 , loss avg over log interval:  3.003880023956299\n",
            "Epoch:  75 , Batch:  49 , loss avg over log interval:  3.005595703125\n",
            "Epoch:  75 , Batch:  99 , loss avg over log interval:  3.0393283104896547\n",
            "Epoch:  75 , avg total loss:  3.0363608106322912\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.84525947454499\n",
            "Accuracy:  0.5249757045675413\n",
            "Epoch 75/249 Completed\n",
            "Epoch:  76 , Batch:  0 , loss avg over log interval:  3.00384259223938\n",
            "Epoch:  76 , Batch:  49 , loss avg over log interval:  3.005067641735077\n",
            "Epoch:  76 , Batch:  99 , loss avg over log interval:  3.040507798194885\n",
            "Epoch:  76 , avg total loss:  3.0363541651463164\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845520432402448\n",
            "Accuracy:  0.5251700680272109\n",
            "Epoch 76/249 Completed\n",
            "Epoch:  77 , Batch:  0 , loss avg over log interval:  3.002298593521118\n",
            "Epoch:  77 , Batch:  49 , loss avg over log interval:  3.0037396359443664\n",
            "Epoch:  77 , Batch:  99 , loss avg over log interval:  3.04045556306839\n",
            "Epoch:  77 , avg total loss:  3.036227635715319\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6484375\n",
            "Avg Validation Loss:  6.845412260148583\n",
            "Accuracy:  0.5249757045675413\n",
            "Epoch 77/249 Completed\n",
            "Epoch:  78 , Batch:  0 , loss avg over log interval:  3.0081684589385986\n",
            "Epoch:  78 , Batch:  49 , loss avg over log interval:  3.0045785665512086\n",
            "Epoch:  78 , Batch:  99 , loss avg over log interval:  3.0394645023345945\n",
            "Epoch:  78 , avg total loss:  3.035664251749066\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.640625\n",
            "Avg Validation Loss:  6.845641880500607\n",
            "Accuracy:  0.5247813411078717\n",
            "Epoch 78/249 Completed\n",
            "Epoch:  79 , Batch:  0 , loss avg over log interval:  3.003704071044922\n",
            "Epoch:  79 , Batch:  49 , loss avg over log interval:  3.003790032863617\n",
            "Epoch:  79 , Batch:  99 , loss avg over log interval:  3.0411872982978823\n",
            "Epoch:  79 , avg total loss:  3.0360169643941135\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.640625\n",
            "Avg Validation Loss:  6.845782245077738\n",
            "Accuracy:  0.524198250728863\n",
            "Epoch 79/249 Completed\n",
            "Epoch:  80 , Batch:  0 , loss avg over log interval:  3.0081045627593994\n",
            "Epoch:  80 , Batch:  49 , loss avg over log interval:  3.0052747416496275\n",
            "Epoch:  80 , Batch:  99 , loss avg over log interval:  3.037581915855408\n",
            "Epoch:  80 , avg total loss:  3.035618759583736\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.640625\n",
            "Avg Validation Loss:  6.846004928030619\n",
            "Accuracy:  0.5243926141885326\n",
            "Epoch 80/249 Completed\n",
            "Epoch:  81 , Batch:  0 , loss avg over log interval:  3.008101463317871\n",
            "Epoch:  81 , Batch:  49 , loss avg over log interval:  3.0049893736839293\n",
            "Epoch:  81 , Batch:  99 , loss avg over log interval:  3.0386662101745605\n",
            "Epoch:  81 , avg total loss:  3.035644717838453\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.640625\n",
            "Avg Validation Loss:  6.846076372193127\n",
            "Accuracy:  0.5240038872691934\n",
            "Epoch 81/249 Completed\n",
            "Epoch:  82 , Batch:  0 , loss avg over log interval:  3.00216007232666\n",
            "Epoch:  82 , Batch:  49 , loss avg over log interval:  3.0033154320716857\n",
            "Epoch:  82 , Batch:  99 , loss avg over log interval:  3.038562581539154\n",
            "Epoch:  82 , avg total loss:  3.0353460985681284\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.640625\n",
            "Avg Validation Loss:  6.846211322923986\n",
            "Accuracy:  0.5240038872691934\n",
            "Epoch 82/249 Completed\n",
            "Epoch:  83 , Batch:  0 , loss avg over log interval:  3.0081212520599365\n",
            "Epoch:  83 , Batch:  49 , loss avg over log interval:  3.0033889985084534\n",
            "Epoch:  83 , Batch:  99 , loss avg over log interval:  3.038382740020752\n",
            "Epoch:  83 , avg total loss:  3.034898445226144\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.640625\n",
            "Avg Validation Loss:  6.846185940067943\n",
            "Accuracy:  0.5234207968901846\n",
            "Epoch 83/249 Completed\n",
            "Epoch:  84 , Batch:  0 , loss avg over log interval:  3.003556251525879\n",
            "Epoch:  84 , Batch:  49 , loss avg over log interval:  3.0039059686660767\n",
            "Epoch:  84 , Batch:  99 , loss avg over log interval:  3.038442466259003\n",
            "Epoch:  84 , avg total loss:  3.0348153744918713\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846632649258869\n",
            "Accuracy:  0.5226433430515063\n",
            "Epoch 84/249 Completed\n",
            "Epoch:  85 , Batch:  0 , loss avg over log interval:  3.0035383701324463\n",
            "Epoch:  85 , Batch:  49 , loss avg over log interval:  3.0032637095451356\n",
            "Epoch:  85 , Batch:  99 , loss avg over log interval:  3.0379157853126526\n",
            "Epoch:  85 , avg total loss:  3.0348734777906667\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.846540730173995\n",
            "Accuracy:  0.5222546161321672\n",
            "Epoch 85/249 Completed\n",
            "Epoch:  86 , Batch:  0 , loss avg over log interval:  3.0035316944122314\n",
            "Epoch:  86 , Batch:  49 , loss avg over log interval:  3.004190082550049\n",
            "Epoch:  86 , Batch:  99 , loss avg over log interval:  3.0379852986335756\n",
            "Epoch:  86 , avg total loss:  3.035028788490572\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.846514672767825\n",
            "Accuracy:  0.5230320699708455\n",
            "Epoch 86/249 Completed\n",
            "Epoch:  87 , Batch:  0 , loss avg over log interval:  3.0019187927246094\n",
            "Epoch:  87 , Batch:  49 , loss avg over log interval:  3.0036671280860903\n",
            "Epoch:  87 , Batch:  99 , loss avg over log interval:  3.038887233734131\n",
            "Epoch:  87 , avg total loss:  3.0350574211797854\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.846505136024661\n",
            "Accuracy:  0.5228377065111759\n",
            "Epoch 87/249 Completed\n",
            "Epoch:  88 , Batch:  0 , loss avg over log interval:  3.003561019897461\n",
            "Epoch:  88 , Batch:  49 , loss avg over log interval:  3.0040291476249696\n",
            "Epoch:  88 , Batch:  99 , loss avg over log interval:  3.0377949047088624\n",
            "Epoch:  88 , avg total loss:  3.0348201679146807\n",
            "iter   0\n",
            "accuracy so far =  0.4296875\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.846666498882015\n",
            "Accuracy:  0.5220602526724976\n",
            "Epoch 88/249 Completed\n",
            "Epoch:  89 , Batch:  0 , loss avg over log interval:  3.0035037994384766\n",
            "Epoch:  89 , Batch:  49 , loss avg over log interval:  3.0027678394317627\n",
            "Epoch:  89 , Batch:  99 , loss avg over log interval:  3.0394100904464723\n",
            "Epoch:  89 , avg total loss:  3.0348539542460786\n",
            "iter   0\n",
            "accuracy so far =  0.421875\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.847139753946444\n",
            "Accuracy:  0.5214771622934888\n",
            "Epoch 89/249 Completed\n",
            "Epoch:  90 , Batch:  0 , loss avg over log interval:  3.0034689903259277\n",
            "Epoch:  90 , Batch:  49 , loss avg over log interval:  3.0042335200309753\n",
            "Epoch:  90 , Batch:  99 , loss avg over log interval:  3.0372926950454713\n",
            "Epoch:  90 , avg total loss:  3.034850001335144\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.847173289554875\n",
            "Accuracy:  0.521865889212828\n",
            "Epoch 90/249 Completed\n",
            "Epoch:  91 , Batch:  0 , loss avg over log interval:  3.008507251739502\n",
            "Epoch:  91 , Batch:  49 , loss avg over log interval:  3.002887856960297\n",
            "Epoch:  91 , Batch:  99 , loss avg over log interval:  3.03671413898468\n",
            "Epoch:  91 , avg total loss:  3.0337873619535696\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.847100705635257\n",
            "Accuracy:  0.5220602526724976\n",
            "Epoch 91/249 Completed\n",
            "Epoch:  92 , Batch:  0 , loss avg over log interval:  3.0084846019744873\n",
            "Epoch:  92 , Batch:  49 , loss avg over log interval:  3.0033637475967407\n",
            "Epoch:  92 , Batch:  99 , loss avg over log interval:  3.0373523473739623\n",
            "Epoch:  92 , avg total loss:  3.034839519556018\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.847291946411133\n",
            "Accuracy:  0.5220602526724976\n",
            "Epoch 92/249 Completed\n",
            "Epoch:  93 , Batch:  0 , loss avg over log interval:  3.008507251739502\n",
            "Epoch:  93 , Batch:  49 , loss avg over log interval:  3.003483707904816\n",
            "Epoch:  93 , Batch:  99 , loss avg over log interval:  3.037741017341614\n",
            "Epoch:  93 , avg total loss:  3.0344687682994897\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.847268127813572\n",
            "Accuracy:  0.521865889212828\n",
            "Epoch 93/249 Completed\n",
            "Epoch:  94 , Batch:  0 , loss avg over log interval:  3.0085222721099854\n",
            "Epoch:  94 , Batch:  49 , loss avg over log interval:  3.0038542628288267\n",
            "Epoch:  94 , Batch:  99 , loss avg over log interval:  3.0369892621040346\n",
            "Epoch:  94 , avg total loss:  3.0342187777809473\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.84746129919843\n",
            "Accuracy:  0.5220602526724976\n",
            "Epoch 94/249 Completed\n",
            "Epoch:  95 , Batch:  0 , loss avg over log interval:  3.003289222717285\n",
            "Epoch:  95 , Batch:  49 , loss avg over log interval:  3.004371962547302\n",
            "Epoch:  95 , Batch:  99 , loss avg over log interval:  3.0377583289146424\n",
            "Epoch:  95 , avg total loss:  3.03482567054638\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.84750472627035\n",
            "Accuracy:  0.5214771622934888\n",
            "Epoch 95/249 Completed\n",
            "Epoch:  96 , Batch:  0 , loss avg over log interval:  3.0085506439208984\n",
            "Epoch:  96 , Batch:  49 , loss avg over log interval:  3.003733401298523\n",
            "Epoch:  96 , Batch:  99 , loss avg over log interval:  3.0373285102844236\n",
            "Epoch:  96 , avg total loss:  3.034416920033054\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.847359139744828\n",
            "Accuracy:  0.5216715257531584\n",
            "Epoch 96/249 Completed\n",
            "Epoch:  97 , Batch:  0 , loss avg over log interval:  3.0033211708068848\n",
            "Epoch:  97 , Batch:  49 , loss avg over log interval:  3.0020155549049377\n",
            "Epoch:  97 , Batch:  99 , loss avg over log interval:  3.0382670879364015\n",
            "Epoch:  97 , avg total loss:  3.0339350233907285\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846993644063065\n",
            "Accuracy:  0.5214771622934888\n",
            "Epoch 97/249 Completed\n",
            "Epoch:  98 , Batch:  0 , loss avg over log interval:  3.008479356765747\n",
            "Epoch:  98 , Batch:  49 , loss avg over log interval:  3.0035214686393736\n",
            "Epoch:  98 , Batch:  99 , loss avg over log interval:  3.0366483879089357\n",
            "Epoch:  98 , avg total loss:  3.0337786881820015\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.847117953184174\n",
            "Accuracy:  0.5216715257531584\n",
            "Epoch 98/249 Completed\n",
            "Epoch:  99 , Batch:  0 , loss avg over log interval:  3.0032052993774414\n",
            "Epoch:  99 , Batch:  49 , loss avg over log interval:  3.002849078178406\n",
            "Epoch:  99 , Batch:  99 , loss avg over log interval:  3.0361384654045107\n",
            "Epoch:  99 , avg total loss:  3.03361688921417\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.84703929831342\n",
            "Accuracy:  0.5216715257531584\n",
            "Epoch 99/249 Completed\n",
            "Epoch:  100 , Batch:  0 , loss avg over log interval:  3.001492977142334\n",
            "Epoch:  100 , Batch:  49 , loss avg over log interval:  3.001222610473633\n",
            "Epoch:  100 , Batch:  99 , loss avg over log interval:  3.036240649223328\n",
            "Epoch:  100 , avg total loss:  3.033141202684762\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846918030482967\n",
            "Accuracy:  0.5214771622934888\n",
            "Epoch 100/249 Completed\n",
            "Epoch:  101 , Batch:  0 , loss avg over log interval:  3.0084424018859863\n",
            "Epoch:  101 , Batch:  49 , loss avg over log interval:  3.00339164018631\n",
            "Epoch:  101 , Batch:  99 , loss avg over log interval:  3.037564175128937\n",
            "Epoch:  101 , avg total loss:  3.0340079518331997\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846772345100961\n",
            "Accuracy:  0.5208940719144801\n",
            "Epoch 101/249 Completed\n",
            "Epoch:  102 , Batch:  0 , loss avg over log interval:  3.0013463497161865\n",
            "Epoch:  102 , Batch:  49 , loss avg over log interval:  3.0026140999794007\n",
            "Epoch:  102 , Batch:  99 , loss avg over log interval:  3.0368863844871523\n",
            "Epoch:  102 , avg total loss:  3.0335397970849187\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846789197223942\n",
            "Accuracy:  0.5206997084548105\n",
            "Epoch 102/249 Completed\n",
            "Epoch:  103 , Batch:  0 , loss avg over log interval:  3.0084805488586426\n",
            "Epoch:  103 , Batch:  49 , loss avg over log interval:  3.002018485069275\n",
            "Epoch:  103 , Batch:  99 , loss avg over log interval:  3.0344272804260255\n",
            "Epoch:  103 , avg total loss:  3.0328699315803638\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846346715601479\n",
            "Accuracy:  0.5205053449951409\n",
            "Epoch 103/249 Completed\n",
            "Epoch:  104 , Batch:  0 , loss avg over log interval:  3.0011532306671143\n",
            "Epoch:  104 , Batch:  49 , loss avg over log interval:  3.0023887753486633\n",
            "Epoch:  104 , Batch:  99 , loss avg over log interval:  3.0345627808570863\n",
            "Epoch:  104 , avg total loss:  3.032650543295819\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846504961572042\n",
            "Accuracy:  0.5208940719144801\n",
            "Epoch 104/249 Completed\n",
            "Epoch:  105 , Batch:  0 , loss avg over log interval:  3.00840163230896\n",
            "Epoch:  105 , Batch:  49 , loss avg over log interval:  3.0029046154022216\n",
            "Epoch:  105 , Batch:  99 , loss avg over log interval:  3.037055470943451\n",
            "Epoch:  105 , avg total loss:  3.03403908100681\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.84648248044456\n",
            "Accuracy:  0.5212827988338192\n",
            "Epoch 105/249 Completed\n",
            "Epoch:  106 , Batch:  0 , loss avg over log interval:  3.0083811283111572\n",
            "Epoch:  106 , Batch:  49 , loss avg over log interval:  3.002562370300293\n",
            "Epoch:  106 , Batch:  99 , loss avg over log interval:  3.036119725704193\n",
            "Epoch:  106 , avg total loss:  3.033327986364779\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846300875268331\n",
            "Accuracy:  0.5212827988338192\n",
            "Epoch 106/249 Completed\n",
            "Epoch:  107 , Batch:  0 , loss avg over log interval:  3.0083975791931152\n",
            "Epoch:  107 , Batch:  49 , loss avg over log interval:  3.002131202220917\n",
            "Epoch:  107 , Batch:  99 , loss avg over log interval:  3.036202595233917\n",
            "Epoch:  107 , avg total loss:  3.033127193001733\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846704163202426\n",
            "Accuracy:  0.5208940719144801\n",
            "Epoch 107/249 Completed\n",
            "Epoch:  108 , Batch:  0 , loss avg over log interval:  3.0084187984466553\n",
            "Epoch:  108 , Batch:  49 , loss avg over log interval:  3.001503758430481\n",
            "Epoch:  108 , Batch:  99 , loss avg over log interval:  3.0366566967964173\n",
            "Epoch:  108 , avg total loss:  3.033105248990266\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846563984708088\n",
            "Accuracy:  0.5210884353741496\n",
            "Epoch 108/249 Completed\n",
            "Epoch:  109 , Batch:  0 , loss avg over log interval:  3.002899408340454\n",
            "Epoch:  109 , Batch:  49 , loss avg over log interval:  3.0032294487953184\n",
            "Epoch:  109 , Batch:  99 , loss avg over log interval:  3.0356202912330628\n",
            "Epoch:  109 , avg total loss:  3.0334304476129836\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846500827044975\n",
            "Accuracy:  0.5210884353741496\n",
            "Epoch 109/249 Completed\n",
            "Epoch:  110 , Batch:  0 , loss avg over log interval:  3.002925157546997\n",
            "Epoch:  110 , Batch:  49 , loss avg over log interval:  3.0020772004127503\n",
            "Epoch:  110 , Batch:  99 , loss avg over log interval:  3.0354264998435974\n",
            "Epoch:  110 , avg total loss:  3.0328600493030273\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846326857078366\n",
            "Accuracy:  0.5206997084548105\n",
            "Epoch 110/249 Completed\n",
            "Epoch:  111 , Batch:  0 , loss avg over log interval:  3.0010905265808105\n",
            "Epoch:  111 , Batch:  49 , loss avg over log interval:  3.001623077392578\n",
            "Epoch:  111 , Batch:  99 , loss avg over log interval:  3.0353876638412474\n",
            "Epoch:  111 , avg total loss:  3.0323913080104883\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846325926664399\n",
            "Accuracy:  0.5210884353741496\n",
            "Epoch 111/249 Completed\n",
            "Epoch:  112 , Batch:  0 , loss avg over log interval:  3.001030683517456\n",
            "Epoch:  112 , Batch:  49 , loss avg over log interval:  3.0018170499801635\n",
            "Epoch:  112 , Batch:  99 , loss avg over log interval:  3.036412515640259\n",
            "Epoch:  112 , avg total loss:  3.0330445369084678\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.8463725287739825\n",
            "Accuracy:  0.5212827988338192\n",
            "Epoch 112/249 Completed\n",
            "Epoch:  113 , Batch:  0 , loss avg over log interval:  3.002856731414795\n",
            "Epoch:  113 , Batch:  49 , loss avg over log interval:  3.002742054462433\n",
            "Epoch:  113 , Batch:  99 , loss avg over log interval:  3.035741438865662\n",
            "Epoch:  113 , avg total loss:  3.0335532176321833\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846434738577866\n",
            "Accuracy:  0.5216715257531584\n",
            "Epoch 113/249 Completed\n",
            "Epoch:  114 , Batch:  0 , loss avg over log interval:  3.00844144821167\n",
            "Epoch:  114 , Batch:  49 , loss avg over log interval:  3.0014877128601074\n",
            "Epoch:  114 , Batch:  99 , loss avg over log interval:  3.034935922622681\n",
            "Epoch:  114 , avg total loss:  3.0323003357735234\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846577155880812\n",
            "Accuracy:  0.5206997084548105\n",
            "Epoch 114/249 Completed\n",
            "Epoch:  115 , Batch:  0 , loss avg over log interval:  3.0084431171417236\n",
            "Epoch:  115 , Batch:  49 , loss avg over log interval:  3.000733108520508\n",
            "Epoch:  115 , Batch:  99 , loss avg over log interval:  3.0365450286865237\n",
            "Epoch:  115 , avg total loss:  3.0326067744821743\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.8465298792211025\n",
            "Accuracy:  0.5206997084548105\n",
            "Epoch 115/249 Completed\n",
            "Epoch:  116 , Batch:  0 , loss avg over log interval:  3.0028011798858643\n",
            "Epoch:  116 , Batch:  49 , loss avg over log interval:  3.001617624759674\n",
            "Epoch:  116 , Batch:  99 , loss avg over log interval:  3.036643271446228\n",
            "Epoch:  116 , avg total loss:  3.0325589404589888\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.846625508331671\n",
            "Accuracy:  0.5212827988338192\n",
            "Epoch 116/249 Completed\n",
            "Epoch:  117 , Batch:  0 , loss avg over log interval:  3.0084478855133057\n",
            "Epoch:  117 , Batch:  49 , loss avg over log interval:  3.0013894963264467\n",
            "Epoch:  117 , Batch:  99 , loss avg over log interval:  3.0361754798889162\n",
            "Epoch:  117 , avg total loss:  3.0326782715493352\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846467983431932\n",
            "Accuracy:  0.5214771622934888\n",
            "Epoch 117/249 Completed\n",
            "Epoch:  118 , Batch:  0 , loss avg over log interval:  3.0007076263427734\n",
            "Epoch:  118 , Batch:  49 , loss avg over log interval:  3.00348694562912\n",
            "Epoch:  118 , Batch:  99 , loss avg over log interval:  3.0349094605445863\n",
            "Epoch:  118 , avg total loss:  3.0327731569608054\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846625392029925\n",
            "Accuracy:  0.5212827988338192\n",
            "Epoch 118/249 Completed\n",
            "Epoch:  119 , Batch:  0 , loss avg over log interval:  3.002650499343872\n",
            "Epoch:  119 , Batch:  49 , loss avg over log interval:  3.001605660915375\n",
            "Epoch:  119 , Batch:  99 , loss avg over log interval:  3.036091582775116\n",
            "Epoch:  119 , avg total loss:  3.0322539581768755\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846324792722377\n",
            "Accuracy:  0.5216715257531584\n",
            "Epoch 119/249 Completed\n",
            "Epoch:  120 , Batch:  0 , loss avg over log interval:  3.002612352371216\n",
            "Epoch:  120 , Batch:  49 , loss avg over log interval:  3.0018035340309144\n",
            "Epoch:  120 , Batch:  99 , loss avg over log interval:  3.0358846044540404\n",
            "Epoch:  120 , avg total loss:  3.032449800035228\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846446642061559\n",
            "Accuracy:  0.5212827988338192\n",
            "Epoch 120/249 Completed\n",
            "Epoch:  121 , Batch:  0 , loss avg over log interval:  3.0005013942718506\n",
            "Epoch:  121 , Batch:  49 , loss avg over log interval:  3.001467878818512\n",
            "Epoch:  121 , Batch:  99 , loss avg over log interval:  3.036362314224243\n",
            "Epoch:  121 , avg total loss:  3.0328024643054907\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846334562069032\n",
            "Accuracy:  0.5212827988338192\n",
            "Epoch 121/249 Completed\n",
            "Epoch:  122 , Batch:  0 , loss avg over log interval:  3.0084011554718018\n",
            "Epoch:  122 , Batch:  49 , loss avg over log interval:  3.001936113834381\n",
            "Epoch:  122 , Batch:  99 , loss avg over log interval:  3.0353098130226135\n",
            "Epoch:  122 , avg total loss:  3.0325838647026946\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846660689609807\n",
            "Accuracy:  0.5206997084548105\n",
            "Epoch 122/249 Completed\n",
            "Epoch:  123 , Batch:  0 , loss avg over log interval:  3.008345127105713\n",
            "Epoch:  123 , Batch:  49 , loss avg over log interval:  3.001057593822479\n",
            "Epoch:  123 , Batch:  99 , loss avg over log interval:  3.0343619990348816\n",
            "Epoch:  123 , avg total loss:  3.031955583371978\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.847075747280586\n",
            "Accuracy:  0.5206997084548105\n",
            "Epoch 123/249 Completed\n",
            "Epoch:  124 , Batch:  0 , loss avg over log interval:  3.008293390274048\n",
            "Epoch:  124 , Batch:  49 , loss avg over log interval:  3.0012768721580505\n",
            "Epoch:  124 , Batch:  99 , loss avg over log interval:  3.03595977306366\n",
            "Epoch:  124 , avg total loss:  3.032298930313276\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.846652624083728\n",
            "Accuracy:  0.5212827988338192\n",
            "Epoch 124/249 Completed\n",
            "Epoch:  125 , Batch:  0 , loss avg over log interval:  3.0001320838928223\n",
            "Epoch:  125 , Batch:  49 , loss avg over log interval:  3.0014537501335146\n",
            "Epoch:  125 , Batch:  99 , loss avg over log interval:  3.0347595238685607\n",
            "Epoch:  125 , avg total loss:  3.032120248545771\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846455998537017\n",
            "Accuracy:  0.5210884353741496\n",
            "Epoch 125/249 Completed\n",
            "Epoch:  126 , Batch:  0 , loss avg over log interval:  3.0021731853485107\n",
            "Epoch:  126 , Batch:  49 , loss avg over log interval:  3.0021621227264403\n",
            "Epoch:  126 , Batch:  99 , loss avg over log interval:  3.034336869716644\n",
            "Epoch:  126 , avg total loss:  3.032185467257016\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.846826181179139\n",
            "Accuracy:  0.5210884353741496\n",
            "Epoch 126/249 Completed\n",
            "Epoch:  127 , Batch:  0 , loss avg over log interval:  3.000028133392334\n",
            "Epoch:  127 , Batch:  49 , loss avg over log interval:  3.0011490035057067\n",
            "Epoch:  127 , Batch:  99 , loss avg over log interval:  3.035724582672119\n",
            "Epoch:  127 , avg total loss:  3.031916024028391\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.84688012192889\n",
            "Accuracy:  0.5205053449951409\n",
            "Epoch 127/249 Completed\n",
            "Epoch:  128 , Batch:  0 , loss avg over log interval:  3.0000360012054443\n",
            "Epoch:  128 , Batch:  49 , loss avg over log interval:  3.0001336526870728\n",
            "Epoch:  128 , Batch:  99 , loss avg over log interval:  3.035000228881836\n",
            "Epoch:  128 , avg total loss:  3.0314534289249475\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.846818918135108\n",
            "Accuracy:  0.5201166180758018\n",
            "Epoch 128/249 Completed\n",
            "Epoch:  129 , Batch:  0 , loss avg over log interval:  3.002159833908081\n",
            "Epoch:  129 , Batch:  49 , loss avg over log interval:  3.001900541782379\n",
            "Epoch:  129 , Batch:  99 , loss avg over log interval:  3.033968195915222\n",
            "Epoch:  129 , avg total loss:  3.0318312342616096\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846909267146413\n",
            "Accuracy:  0.5210884353741496\n",
            "Epoch 129/249 Completed\n",
            "Epoch:  130 , Batch:  0 , loss avg over log interval:  2.9999325275421143\n",
            "Epoch:  130 , Batch:  49 , loss avg over log interval:  3.0011788964271546\n",
            "Epoch:  130 , Batch:  99 , loss avg over log interval:  3.035226275920868\n",
            "Epoch:  130 , avg total loss:  3.031834356162859\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.8466630563503355\n",
            "Accuracy:  0.5212827988338192\n",
            "Epoch 130/249 Completed\n",
            "Epoch:  131 , Batch:  0 , loss avg over log interval:  3.002042531967163\n",
            "Epoch:  131 , Batch:  49 , loss avg over log interval:  3.000849039554596\n",
            "Epoch:  131 , Batch:  99 , loss avg over log interval:  3.034452896118164\n",
            "Epoch:  131 , avg total loss:  3.0312949937322866\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.84621157297274\n",
            "Accuracy:  0.5216715257531584\n",
            "Epoch 131/249 Completed\n",
            "Epoch:  132 , Batch:  0 , loss avg over log interval:  2.9998230934143066\n",
            "Epoch:  132 , Batch:  49 , loss avg over log interval:  3.000003604888916\n",
            "Epoch:  132 , Batch:  99 , loss avg over log interval:  3.033772096633911\n",
            "Epoch:  132 , avg total loss:  3.0313735664754673\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.846102952957153\n",
            "Accuracy:  0.5224489795918368\n",
            "Epoch 132/249 Completed\n",
            "Epoch:  133 , Batch:  0 , loss avg over log interval:  3.001826763153076\n",
            "Epoch:  133 , Batch:  49 , loss avg over log interval:  3.001981489658356\n",
            "Epoch:  133 , Batch:  99 , loss avg over log interval:  3.034219546318054\n",
            "Epoch:  133 , avg total loss:  3.0318131360454834\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.846160696773994\n",
            "Accuracy:  0.5216715257531584\n",
            "Epoch 133/249 Completed\n",
            "Epoch:  134 , Batch:  0 , loss avg over log interval:  3.0017690658569336\n",
            "Epoch:  134 , Batch:  49 , loss avg over log interval:  2.9999139595031736\n",
            "Epoch:  134 , Batch:  99 , loss avg over log interval:  3.0340643382072447\n",
            "Epoch:  134 , avg total loss:  3.0309592805046965\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.846055972866896\n",
            "Accuracy:  0.5214771622934888\n",
            "Epoch 134/249 Completed\n",
            "Epoch:  135 , Batch:  0 , loss avg over log interval:  3.0017025470733643\n",
            "Epoch:  135 , Batch:  49 , loss avg over log interval:  3.0009428191185\n",
            "Epoch:  135 , Batch:  99 , loss avg over log interval:  3.0351141715049743\n",
            "Epoch:  135 , avg total loss:  3.031298340230748\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.846167203856678\n",
            "Accuracy:  0.5216715257531584\n",
            "Epoch 135/249 Completed\n",
            "Epoch:  136 , Batch:  0 , loss avg over log interval:  3.0016088485717773\n",
            "Epoch:  136 , Batch:  49 , loss avg over log interval:  3.000793933868408\n",
            "Epoch:  136 , Batch:  99 , loss avg over log interval:  3.0338619112968446\n",
            "Epoch:  136 , avg total loss:  3.0306678543920103\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.846127341433269\n",
            "Accuracy:  0.5220602526724976\n",
            "Epoch 136/249 Completed\n",
            "Epoch:  137 , Batch:  0 , loss avg over log interval:  2.999312162399292\n",
            "Epoch:  137 , Batch:  49 , loss avg over log interval:  3.000721971988678\n",
            "Epoch:  137 , Batch:  99 , loss avg over log interval:  3.0352865648269653\n",
            "Epoch:  137 , avg total loss:  3.031633246636045\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846212817401421\n",
            "Accuracy:  0.5216715257531584\n",
            "Epoch 137/249 Completed\n",
            "Epoch:  138 , Batch:  0 , loss avg over log interval:  2.9992990493774414\n",
            "Epoch:  138 , Batch:  49 , loss avg over log interval:  3.001562685966492\n",
            "Epoch:  138 , Batch:  99 , loss avg over log interval:  3.035566086769104\n",
            "Epoch:  138 , avg total loss:  3.031768078389375\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846558489450595\n",
            "Accuracy:  0.521865889212828\n",
            "Epoch 138/249 Completed\n",
            "Epoch:  139 , Batch:  0 , loss avg over log interval:  2.9993343353271484\n",
            "Epoch:  139 , Batch:  49 , loss avg over log interval:  2.999795050621033\n",
            "Epoch:  139 , Batch:  99 , loss avg over log interval:  3.0352273654937743\n",
            "Epoch:  139 , avg total loss:  3.0309571762015852\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846578295637921\n",
            "Accuracy:  0.5214771622934888\n",
            "Epoch 139/249 Completed\n",
            "Epoch:  140 , Batch:  0 , loss avg over log interval:  3.0015575885772705\n",
            "Epoch:  140 , Batch:  49 , loss avg over log interval:  3.000504515171051\n",
            "Epoch:  140 , Batch:  99 , loss avg over log interval:  3.0352414989471437\n",
            "Epoch:  140 , avg total loss:  3.031414220298546\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.846798925864987\n",
            "Accuracy:  0.5208940719144801\n",
            "Epoch 140/249 Completed\n",
            "Epoch:  141 , Batch:  0 , loss avg over log interval:  3.001477003097534\n",
            "Epoch:  141 , Batch:  49 , loss avg over log interval:  2.9999512696266173\n",
            "Epoch:  141 , Batch:  99 , loss avg over log interval:  3.0337261319160462\n",
            "Epoch:  141 , avg total loss:  3.0304923187131467\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846841963326058\n",
            "Accuracy:  0.5203109815354713\n",
            "Epoch 141/249 Completed\n",
            "Epoch:  142 , Batch:  0 , loss avg over log interval:  2.9990954399108887\n",
            "Epoch:  142 , Batch:  49 , loss avg over log interval:  3.000706145763397\n",
            "Epoch:  142 , Batch:  99 , loss avg over log interval:  3.0351739954948425\n",
            "Epoch:  142 , avg total loss:  3.031927610653034\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846729796107223\n",
            "Accuracy:  0.5203109815354713\n",
            "Epoch 142/249 Completed\n",
            "Epoch:  143 , Batch:  0 , loss avg over log interval:  3.0014069080352783\n",
            "Epoch:  143 , Batch:  49 , loss avg over log interval:  2.9997121596336367\n",
            "Epoch:  143 , Batch:  99 , loss avg over log interval:  3.034754681587219\n",
            "Epoch:  143 , avg total loss:  3.030885762926461\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846627311008732\n",
            "Accuracy:  0.5203109815354713\n",
            "Epoch 143/249 Completed\n",
            "Epoch:  144 , Batch:  0 , loss avg over log interval:  2.998889207839966\n",
            "Epoch:  144 , Batch:  49 , loss avg over log interval:  2.9992030692100524\n",
            "Epoch:  144 , Batch:  99 , loss avg over log interval:  3.033819558620453\n",
            "Epoch:  144 , avg total loss:  3.0304460041764854\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846468745208368\n",
            "Accuracy:  0.5212827988338192\n",
            "Epoch 144/249 Completed\n",
            "Epoch:  145 , Batch:  0 , loss avg over log interval:  3.0011870861053467\n",
            "Epoch:  145 , Batch:  49 , loss avg over log interval:  2.999425573348999\n",
            "Epoch:  145 , Batch:  99 , loss avg over log interval:  3.0336348176002503\n",
            "Epoch:  145 , avg total loss:  3.0309718475825544\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846403523189266\n",
            "Accuracy:  0.5212827988338192\n",
            "Epoch 145/249 Completed\n",
            "Epoch:  146 , Batch:  0 , loss avg over log interval:  3.0010740756988525\n",
            "Epoch:  146 , Batch:  49 , loss avg over log interval:  3.0006988883018493\n",
            "Epoch:  146 , Batch:  99 , loss avg over log interval:  3.0336965894699097\n",
            "Epoch:  146 , avg total loss:  3.0308499215305718\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.846538177350673\n",
            "Accuracy:  0.5206997084548105\n",
            "Epoch 146/249 Completed\n",
            "Epoch:  147 , Batch:  0 , loss avg over log interval:  3.0079612731933594\n",
            "Epoch:  147 , Batch:  49 , loss avg over log interval:  3.0012864375114443\n",
            "Epoch:  147 , Batch:  99 , loss avg over log interval:  3.033724067211151\n",
            "Epoch:  147 , avg total loss:  3.0316814797512\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.846534583626724\n",
            "Accuracy:  0.5210884353741496\n",
            "Epoch 147/249 Completed\n",
            "Epoch:  148 , Batch:  0 , loss avg over log interval:  3.0010292530059814\n",
            "Epoch:  148 , Batch:  49 , loss avg over log interval:  2.9992079639434817\n",
            "Epoch:  148 , Batch:  99 , loss avg over log interval:  3.034451336860657\n",
            "Epoch:  148 , avg total loss:  3.03073283209317\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.846442990186738\n",
            "Accuracy:  0.5210884353741496\n",
            "Epoch 148/249 Completed\n",
            "Epoch:  149 , Batch:  0 , loss avg over log interval:  3.007863998413086\n",
            "Epoch:  149 , Batch:  49 , loss avg over log interval:  3.000477955341339\n",
            "Epoch:  149 , Batch:  99 , loss avg over log interval:  3.0340321135520933\n",
            "Epoch:  149 , avg total loss:  3.0307974003363345\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.846509834615196\n",
            "Accuracy:  0.5214771622934888\n",
            "Epoch 149/249 Completed\n",
            "Epoch:  150 , Batch:  0 , loss avg over log interval:  3.000840902328491\n",
            "Epoch:  150 , Batch:  49 , loss avg over log interval:  2.9995471477508544\n",
            "Epoch:  150 , Batch:  99 , loss avg over log interval:  3.032435743808746\n",
            "Epoch:  150 , avg total loss:  3.0301416758177937\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.846211258958026\n",
            "Accuracy:  0.5212827988338192\n",
            "Epoch 150/249 Completed\n",
            "Epoch:  151 , Batch:  0 , loss avg over log interval:  3.0077950954437256\n",
            "Epoch:  151 , Batch:  49 , loss avg over log interval:  3.0015178227424624\n",
            "Epoch:  151 , Batch:  99 , loss avg over log interval:  3.0344995617866517\n",
            "Epoch:  151 , avg total loss:  3.031155396198881\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846031555315343\n",
            "Accuracy:  0.5214771622934888\n",
            "Epoch 151/249 Completed\n",
            "Epoch:  152 , Batch:  0 , loss avg over log interval:  3.007722854614258\n",
            "Epoch:  152 , Batch:  49 , loss avg over log interval:  3.001456987857819\n",
            "Epoch:  152 , Batch:  99 , loss avg over log interval:  3.0343834590911865\n",
            "Epoch:  152 , avg total loss:  3.0316051002861797\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846141349978563\n",
            "Accuracy:  0.5212827988338192\n",
            "Epoch 152/249 Completed\n",
            "Epoch:  153 , Batch:  0 , loss avg over log interval:  3.0007567405700684\n",
            "Epoch:  153 , Batch:  49 , loss avg over log interval:  3.000388083457947\n",
            "Epoch:  153 , Batch:  99 , loss avg over log interval:  3.033558712005615\n",
            "Epoch:  153 , avg total loss:  3.0305357791375425\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846475717498035\n",
            "Accuracy:  0.5203109815354713\n",
            "Epoch 153/249 Completed\n",
            "Epoch:  154 , Batch:  0 , loss avg over log interval:  3.000816583633423\n",
            "Epoch:  154 , Batch:  49 , loss avg over log interval:  3.000193998813629\n",
            "Epoch:  154 , Batch:  99 , loss avg over log interval:  3.03282879114151\n",
            "Epoch:  154 , avg total loss:  3.030467526636262\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.8462206270636585\n",
            "Accuracy:  0.5205053449951409\n",
            "Epoch 154/249 Completed\n",
            "Epoch:  155 , Batch:  0 , loss avg over log interval:  3.007749080657959\n",
            "Epoch:  155 , Batch:  49 , loss avg over log interval:  2.9998379850387575\n",
            "Epoch:  155 , Batch:  99 , loss avg over log interval:  3.033563075065613\n",
            "Epoch:  155 , avg total loss:  3.030128002166748\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846248684859857\n",
            "Accuracy:  0.5206997084548105\n",
            "Epoch 155/249 Completed\n",
            "Epoch:  156 , Batch:  0 , loss avg over log interval:  2.9984073638916016\n",
            "Epoch:  156 , Batch:  49 , loss avg over log interval:  3.001153037548065\n",
            "Epoch:  156 , Batch:  99 , loss avg over log interval:  3.0340010809898375\n",
            "Epoch:  156 , avg total loss:  3.0315140934957974\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.8464389021803695\n",
            "Accuracy:  0.5206997084548105\n",
            "Epoch 156/249 Completed\n",
            "Epoch:  157 , Batch:  0 , loss avg over log interval:  3.0008327960968018\n",
            "Epoch:  157 , Batch:  49 , loss avg over log interval:  3.0003037667274475\n",
            "Epoch:  157 , Batch:  99 , loss avg over log interval:  3.0332950592041015\n",
            "Epoch:  157 , avg total loss:  3.0306569925252944\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846477601586319\n",
            "Accuracy:  0.5206997084548105\n",
            "Epoch 157/249 Completed\n",
            "Epoch:  158 , Batch:  0 , loss avg over log interval:  3.000821590423584\n",
            "Epoch:  158 , Batch:  49 , loss avg over log interval:  2.999314155578613\n",
            "Epoch:  158 , Batch:  99 , loss avg over log interval:  3.0326839518547057\n",
            "Epoch:  158 , avg total loss:  3.030097805071568\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.846798495548527\n",
            "Accuracy:  0.5210884353741496\n",
            "Epoch 158/249 Completed\n",
            "Epoch:  159 , Batch:  0 , loss avg over log interval:  3.0077855587005615\n",
            "Epoch:  159 , Batch:  49 , loss avg over log interval:  3.0000022578239443\n",
            "Epoch:  159 , Batch:  99 , loss avg over log interval:  3.032354762554169\n",
            "Epoch:  159 , avg total loss:  3.030272742976313\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846863380292567\n",
            "Accuracy:  0.5199222546161322\n",
            "Epoch 159/249 Completed\n",
            "Epoch:  160 , Batch:  0 , loss avg over log interval:  3.000786304473877\n",
            "Epoch:  160 , Batch:  49 , loss avg over log interval:  3.001859667301178\n",
            "Epoch:  160 , Batch:  99 , loss avg over log interval:  3.0327844166755678\n",
            "Epoch:  160 , avg total loss:  3.03102380555609\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846922641847192\n",
            "Accuracy:  0.5205053449951409\n",
            "Epoch 160/249 Completed\n",
            "Epoch:  161 , Batch:  0 , loss avg over log interval:  3.007732629776001\n",
            "Epoch:  161 , Batch:  49 , loss avg over log interval:  2.9996198534965517\n",
            "Epoch:  161 , Batch:  99 , loss avg over log interval:  3.0334228801727297\n",
            "Epoch:  161 , avg total loss:  3.0298241705134297\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846651472696444\n",
            "Accuracy:  0.5205053449951409\n",
            "Epoch 161/249 Completed\n",
            "Epoch:  162 , Batch:  0 , loss avg over log interval:  2.9982423782348633\n",
            "Epoch:  162 , Batch:  49 , loss avg over log interval:  2.998394634723663\n",
            "Epoch:  162 , Batch:  99 , loss avg over log interval:  3.0320906138420103\n",
            "Epoch:  162 , avg total loss:  3.029077254343724\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846835095707963\n",
            "Accuracy:  0.5205053449951409\n",
            "Epoch 162/249 Completed\n",
            "Epoch:  163 , Batch:  0 , loss avg over log interval:  3.007652759552002\n",
            "Epoch:  163 , Batch:  49 , loss avg over log interval:  2.9989908170700073\n",
            "Epoch:  163 , Batch:  99 , loss avg over log interval:  3.032164852619171\n",
            "Epoch:  163 , avg total loss:  3.029678245385488\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.8468096430708725\n",
            "Accuracy:  0.519533527696793\n",
            "Epoch 163/249 Completed\n",
            "Epoch:  164 , Batch:  0 , loss avg over log interval:  3.0076541900634766\n",
            "Epoch:  164 , Batch:  49 , loss avg over log interval:  2.9992005228996277\n",
            "Epoch:  164 , Batch:  99 , loss avg over log interval:  3.0330094265937806\n",
            "Epoch:  164 , avg total loss:  3.0299931652304055\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.846901550525573\n",
            "Accuracy:  0.519533527696793\n",
            "Epoch 164/249 Completed\n",
            "Epoch:  165 , Batch:  0 , loss avg over log interval:  2.9982123374938965\n",
            "Epoch:  165 , Batch:  49 , loss avg over log interval:  2.999565751552582\n",
            "Epoch:  165 , Batch:  99 , loss avg over log interval:  3.0328376507759094\n",
            "Epoch:  165 , avg total loss:  3.0298916161924168\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.8469363654532085\n",
            "Accuracy:  0.5199222546161322\n",
            "Epoch 165/249 Completed\n",
            "Epoch:  166 , Batch:  0 , loss avg over log interval:  2.998241901397705\n",
            "Epoch:  166 , Batch:  49 , loss avg over log interval:  3.0004372072219847\n",
            "Epoch:  166 , Batch:  99 , loss avg over log interval:  3.0321917963027953\n",
            "Epoch:  166 , avg total loss:  3.030284443627233\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.847267877764818\n",
            "Accuracy:  0.5199222546161322\n",
            "Epoch 166/249 Completed\n",
            "Epoch:  167 , Batch:  0 , loss avg over log interval:  3.0076191425323486\n",
            "Epoch:  167 , Batch:  49 , loss avg over log interval:  3.0002350974082947\n",
            "Epoch:  167 , Batch:  99 , loss avg over log interval:  3.0329662561416626\n",
            "Epoch:  167 , avg total loss:  3.0305464777393616\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.847009961198016\n",
            "Accuracy:  0.5201166180758018\n",
            "Epoch 167/249 Completed\n",
            "Epoch:  168 , Batch:  0 , loss avg over log interval:  2.998291492462158\n",
            "Epoch:  168 , Batch:  49 , loss avg over log interval:  2.9992347526550294\n",
            "Epoch:  168 , Batch:  99 , loss avg over log interval:  3.0334857177734373\n",
            "Epoch:  168 , avg total loss:  3.0298582341360007\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.8469895735019595\n",
            "Accuracy:  0.5203109815354713\n",
            "Epoch 168/249 Completed\n",
            "Epoch:  169 , Batch:  0 , loss avg over log interval:  3.0007054805755615\n",
            "Epoch:  169 , Batch:  49 , loss avg over log interval:  2.9986175727844238\n",
            "Epoch:  169 , Batch:  99 , loss avg over log interval:  3.0331424069404602\n",
            "Epoch:  169 , avg total loss:  3.030184507369995\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846757301470128\n",
            "Accuracy:  0.5203109815354713\n",
            "Epoch 169/249 Completed\n",
            "Epoch:  170 , Batch:  0 , loss avg over log interval:  3.0075695514678955\n",
            "Epoch:  170 , Batch:  49 , loss avg over log interval:  2.999805915355682\n",
            "Epoch:  170 , Batch:  99 , loss avg over log interval:  3.033526406288147\n",
            "Epoch:  170 , avg total loss:  3.0307367884594463\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846889908720807\n",
            "Accuracy:  0.5203109815354713\n",
            "Epoch 170/249 Completed\n",
            "Epoch:  171 , Batch:  0 , loss avg over log interval:  3.007479667663574\n",
            "Epoch:  171 , Batch:  49 , loss avg over log interval:  2.9999440360069274\n",
            "Epoch:  171 , Batch:  99 , loss avg over log interval:  3.0335205483436583\n",
            "Epoch:  171 , avg total loss:  3.0305004163064817\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846711414616283\n",
            "Accuracy:  0.5201166180758018\n",
            "Epoch 171/249 Completed\n",
            "Epoch:  172 , Batch:  0 , loss avg over log interval:  2.99824595451355\n",
            "Epoch:  172 , Batch:  49 , loss avg over log interval:  2.9988448548316957\n",
            "Epoch:  172 , Batch:  99 , loss avg over log interval:  3.033403151035309\n",
            "Epoch:  172 , avg total loss:  3.029841664908589\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846658136786484\n",
            "Accuracy:  0.5197278911564626\n",
            "Epoch 172/249 Completed\n",
            "Epoch:  173 , Batch:  0 , loss avg over log interval:  3.007437229156494\n",
            "Epoch:  173 , Batch:  49 , loss avg over log interval:  2.999687340259552\n",
            "Epoch:  173 , Batch:  99 , loss avg over log interval:  3.0337094831466676\n",
            "Epoch:  173 , avg total loss:  3.0302149014196536\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846630893102506\n",
            "Accuracy:  0.5201166180758018\n",
            "Epoch 173/249 Completed\n",
            "Epoch:  174 , Batch:  0 , loss avg over log interval:  2.998262882232666\n",
            "Epoch:  174 , Batch:  49 , loss avg over log interval:  2.999072103500366\n",
            "Epoch:  174 , Batch:  99 , loss avg over log interval:  3.0323565864562987\n",
            "Epoch:  174 , avg total loss:  3.029432745947354\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846851215129945\n",
            "Accuracy:  0.5205053449951409\n",
            "Epoch 174/249 Completed\n",
            "Epoch:  175 , Batch:  0 , loss avg over log interval:  3.007477283477783\n",
            "Epoch:  175 , Batch:  49 , loss avg over log interval:  2.9993510818481446\n",
            "Epoch:  175 , Batch:  99 , loss avg over log interval:  3.0334560465812683\n",
            "Epoch:  175 , avg total loss:  3.0296816255735313\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846807968325731\n",
            "Accuracy:  0.5203109815354713\n",
            "Epoch 175/249 Completed\n",
            "Epoch:  176 , Batch:  0 , loss avg over log interval:  3.0073485374450684\n",
            "Epoch:  176 , Batch:  49 , loss avg over log interval:  2.999367604255676\n",
            "Epoch:  176 , Batch:  99 , loss avg over log interval:  3.033212957382202\n",
            "Epoch:  176 , avg total loss:  3.0296634150587995\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846987706858937\n",
            "Accuracy:  0.5197278911564626\n",
            "Epoch 176/249 Completed\n",
            "Epoch:  177 , Batch:  0 , loss avg over log interval:  2.9980058670043945\n",
            "Epoch:  177 , Batch:  49 , loss avg over log interval:  3.0001014637947083\n",
            "Epoch:  177 , Batch:  99 , loss avg over log interval:  3.0323812437057494\n",
            "Epoch:  177 , avg total loss:  3.0300572652747664\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846910005662499\n",
            "Accuracy:  0.5199222546161322\n",
            "Epoch 177/249 Completed\n",
            "Epoch:  178 , Batch:  0 , loss avg over log interval:  3.000521183013916\n",
            "Epoch:  178 , Batch:  49 , loss avg over log interval:  2.9992152142524717\n",
            "Epoch:  178 , Batch:  99 , loss avg over log interval:  3.033632307052612\n",
            "Epoch:  178 , avg total loss:  3.0299390139787095\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.847200190148702\n",
            "Accuracy:  0.519533527696793\n",
            "Epoch 178/249 Completed\n",
            "Epoch:  179 , Batch:  0 , loss avg over log interval:  2.997955322265625\n",
            "Epoch:  179 , Batch:  49 , loss avg over log interval:  2.999721667766571\n",
            "Epoch:  179 , Batch:  99 , loss avg over log interval:  3.031862323284149\n",
            "Epoch:  179 , avg total loss:  3.0294683921164363\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.847355202930729\n",
            "Accuracy:  0.5191448007774538\n",
            "Epoch 179/249 Completed\n",
            "Epoch:  180 , Batch:  0 , loss avg over log interval:  2.997840404510498\n",
            "Epoch:  180 , Batch:  49 , loss avg over log interval:  2.99997006893158\n",
            "Epoch:  180 , Batch:  99 , loss avg over log interval:  3.033751347064972\n",
            "Epoch:  180 , avg total loss:  3.0303819810134778\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.84731727693139\n",
            "Accuracy:  0.5193391642371235\n",
            "Epoch 180/249 Completed\n",
            "Epoch:  181 , Batch:  0 , loss avg over log interval:  3.007282018661499\n",
            "Epoch:  181 , Batch:  49 , loss avg over log interval:  2.9988423466682432\n",
            "Epoch:  181 , Batch:  99 , loss avg over log interval:  3.0332045102119447\n",
            "Epoch:  181 , avg total loss:  3.029889202636221\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.847534034310318\n",
            "Accuracy:  0.519533527696793\n",
            "Epoch 181/249 Completed\n",
            "Epoch:  182 , Batch:  0 , loss avg over log interval:  3.007258176803589\n",
            "Epoch:  182 , Batch:  49 , loss avg over log interval:  2.998858597278595\n",
            "Epoch:  182 , Batch:  99 , loss avg over log interval:  3.0339817690849302\n",
            "Epoch:  182 , avg total loss:  3.029952247073685\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.847242105297926\n",
            "Accuracy:  0.519533527696793\n",
            "Epoch 182/249 Completed\n",
            "Epoch:  183 , Batch:  0 , loss avg over log interval:  2.9978439807891846\n",
            "Epoch:  183 , Batch:  49 , loss avg over log interval:  3.000834438800812\n",
            "Epoch:  183 , Batch:  99 , loss avg over log interval:  3.032564256191254\n",
            "Epoch:  183 , avg total loss:  3.030118728029555\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.846965336218113\n",
            "Accuracy:  0.519533527696793\n",
            "Epoch 183/249 Completed\n",
            "Epoch:  184 , Batch:  0 , loss avg over log interval:  2.9976608753204346\n",
            "Epoch:  184 , Batch:  49 , loss avg over log interval:  2.99994384765625\n",
            "Epoch:  184 , Batch:  99 , loss avg over log interval:  3.031812746524811\n",
            "Epoch:  184 , avg total loss:  3.0292059139928957\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847145999350199\n",
            "Accuracy:  0.5191448007774538\n",
            "Epoch 184/249 Completed\n",
            "Epoch:  185 , Batch:  0 , loss avg over log interval:  3.0002193450927734\n",
            "Epoch:  185 , Batch:  49 , loss avg over log interval:  2.9981334519386293\n",
            "Epoch:  185 , Batch:  99 , loss avg over log interval:  3.0334334540367127\n",
            "Epoch:  185 , avg total loss:  3.029518619827602\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.847473586477885\n",
            "Accuracy:  0.5187560738581146\n",
            "Epoch 185/249 Completed\n",
            "Epoch:  186 , Batch:  0 , loss avg over log interval:  2.9974637031555176\n",
            "Epoch:  186 , Batch:  49 , loss avg over log interval:  2.998557679653168\n",
            "Epoch:  186 , Batch:  99 , loss avg over log interval:  3.0330270981788634\n",
            "Epoch:  186 , avg total loss:  3.0294072800788325\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.8473545341956905\n",
            "Accuracy:  0.5183673469387755\n",
            "Epoch 186/249 Completed\n",
            "Epoch:  187 , Batch:  0 , loss avg over log interval:  3.0070855617523193\n",
            "Epoch:  187 , Batch:  49 , loss avg over log interval:  3.000366361141205\n",
            "Epoch:  187 , Batch:  99 , loss avg over log interval:  3.0320163798332214\n",
            "Epoch:  187 , avg total loss:  3.0299407520155976\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.84722956215463\n",
            "Accuracy:  0.5185617103984451\n",
            "Epoch 187/249 Completed\n",
            "Epoch:  188 , Batch:  0 , loss avg over log interval:  2.99735164642334\n",
            "Epoch:  188 , Batch:  49 , loss avg over log interval:  3.0006157517433167\n",
            "Epoch:  188 , Batch:  99 , loss avg over log interval:  3.0325809121131897\n",
            "Epoch:  188 , avg total loss:  3.0298363056735718\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847225363661603\n",
            "Accuracy:  0.5183673469387755\n",
            "Epoch 188/249 Completed\n",
            "Epoch:  189 , Batch:  0 , loss avg over log interval:  2.997133731842041\n",
            "Epoch:  189 , Batch:  49 , loss avg over log interval:  3.00037358045578\n",
            "Epoch:  189 , Batch:  99 , loss avg over log interval:  3.031931436061859\n",
            "Epoch:  189 , avg total loss:  3.029786149660746\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847440103205239\n",
            "Accuracy:  0.5175898931000972\n",
            "Epoch 189/249 Completed\n",
            "Epoch:  190 , Batch:  0 , loss avg over log interval:  2.9999842643737793\n",
            "Epoch:  190 , Batch:  49 , loss avg over log interval:  2.9987632393836976\n",
            "Epoch:  190 , Batch:  99 , loss avg over log interval:  3.0327034306526186\n",
            "Epoch:  190 , avg total loss:  3.0298524239788884\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847389180485795\n",
            "Accuracy:  0.5175898931000972\n",
            "Epoch 190/249 Completed\n",
            "Epoch:  191 , Batch:  0 , loss avg over log interval:  3.0069420337677\n",
            "Epoch:  191 , Batch:  49 , loss avg over log interval:  2.998310749530792\n",
            "Epoch:  191 , Batch:  99 , loss avg over log interval:  3.0330767107009886\n",
            "Epoch:  191 , avg total loss:  3.0289410128109697\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.84728445076361\n",
            "Accuracy:  0.5189504373177842\n",
            "Epoch 191/249 Completed\n",
            "Epoch:  192 , Batch:  0 , loss avg over log interval:  2.997117519378662\n",
            "Epoch:  192 , Batch:  49 , loss avg over log interval:  2.99785861492157\n",
            "Epoch:  192 , Batch:  99 , loss avg over log interval:  3.031822943687439\n",
            "Epoch:  192 , avg total loss:  3.0288933653762373\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.84758485235819\n",
            "Accuracy:  0.5179786200194364\n",
            "Epoch 192/249 Completed\n",
            "Epoch:  193 , Batch:  0 , loss avg over log interval:  2.9969263076782227\n",
            "Epoch:  193 , Batch:  49 , loss avg over log interval:  3.0003007435798645\n",
            "Epoch:  193 , Batch:  99 , loss avg over log interval:  3.0315319013595583\n",
            "Epoch:  193 , avg total loss:  3.0294999756674836\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847667740612495\n",
            "Accuracy:  0.5179786200194364\n",
            "Epoch 193/249 Completed\n",
            "Epoch:  194 , Batch:  0 , loss avg over log interval:  2.996976375579834\n",
            "Epoch:  194 , Batch:  49 , loss avg over log interval:  2.9999538135528563\n",
            "Epoch:  194 , Batch:  99 , loss avg over log interval:  3.03299174785614\n",
            "Epoch:  194 , avg total loss:  3.0298843072808306\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847780576566371\n",
            "Accuracy:  0.5181729834791059\n",
            "Epoch 194/249 Completed\n",
            "Epoch:  195 , Batch:  0 , loss avg over log interval:  2.9998064041137695\n",
            "Epoch:  195 , Batch:  49 , loss avg over log interval:  2.998188946247101\n",
            "Epoch:  195 , Batch:  99 , loss avg over log interval:  3.032501187324524\n",
            "Epoch:  195 , avg total loss:  3.028769588124925\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847744557915664\n",
            "Accuracy:  0.5181729834791059\n",
            "Epoch 195/249 Completed\n",
            "Epoch:  196 , Batch:  0 , loss avg over log interval:  2.996910333633423\n",
            "Epoch:  196 , Batch:  49 , loss avg over log interval:  3.0004117131233214\n",
            "Epoch:  196 , Batch:  99 , loss avg over log interval:  3.032825450897217\n",
            "Epoch:  196 , avg total loss:  3.0297889243001523\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847429281327782\n",
            "Accuracy:  0.5179786200194364\n",
            "Epoch 196/249 Completed\n",
            "Epoch:  197 , Batch:  0 , loss avg over log interval:  2.9996542930603027\n",
            "Epoch:  197 , Batch:  49 , loss avg over log interval:  2.998390488624573\n",
            "Epoch:  197 , Batch:  99 , loss avg over log interval:  3.033609209060669\n",
            "Epoch:  197 , avg total loss:  3.029479868170144\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847468422680366\n",
            "Accuracy:  0.5177842565597668\n",
            "Epoch 197/249 Completed\n",
            "Epoch:  198 , Batch:  0 , loss avg over log interval:  2.9967639446258545\n",
            "Epoch:  198 , Batch:  49 , loss avg over log interval:  2.9988967990875244\n",
            "Epoch:  198 , Batch:  99 , loss avg over log interval:  3.032788279056549\n",
            "Epoch:  198 , avg total loss:  3.0293687262396882\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847736283046443\n",
            "Accuracy:  0.5172011661807581\n",
            "Epoch 198/249 Completed\n",
            "Epoch:  199 , Batch:  0 , loss avg over log interval:  2.9966952800750732\n",
            "Epoch:  199 , Batch:  49 , loss avg over log interval:  2.9986025524139404\n",
            "Epoch:  199 , Batch:  99 , loss avg over log interval:  3.033205497264862\n",
            "Epoch:  199 , avg total loss:  3.0293352362038433\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847968043350592\n",
            "Accuracy:  0.5170068027210885\n",
            "Epoch 199/249 Completed\n",
            "Epoch:  200 , Batch:  0 , loss avg over log interval:  3.006742000579834\n",
            "Epoch:  200 , Batch:  49 , loss avg over log interval:  3.000239791870117\n",
            "Epoch:  200 , Batch:  99 , loss avg over log interval:  3.031818804740906\n",
            "Epoch:  200 , avg total loss:  3.029101522072502\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847602884943893\n",
            "Accuracy:  0.5172011661807581\n",
            "Epoch 200/249 Completed\n",
            "Epoch:  201 , Batch:  0 , loss avg over log interval:  2.999577522277832\n",
            "Epoch:  201 , Batch:  49 , loss avg over log interval:  2.999307351112366\n",
            "Epoch:  201 , Batch:  99 , loss avg over log interval:  3.0332749891281128\n",
            "Epoch:  201 , avg total loss:  3.029579001924266\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.8474921482365305\n",
            "Accuracy:  0.5172011661807581\n",
            "Epoch 201/249 Completed\n",
            "Epoch:  202 , Batch:  0 , loss avg over log interval:  2.996750831604004\n",
            "Epoch:  202 , Batch:  49 , loss avg over log interval:  2.9982610034942625\n",
            "Epoch:  202 , Batch:  99 , loss avg over log interval:  3.030850534439087\n",
            "Epoch:  202 , avg total loss:  3.028157146944516\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.84771706999802\n",
            "Accuracy:  0.5170068027210885\n",
            "Epoch 202/249 Completed\n",
            "Epoch:  203 , Batch:  0 , loss avg over log interval:  2.9967565536499023\n",
            "Epoch:  203 , Batch:  49 , loss avg over log interval:  2.999678373336792\n",
            "Epoch:  203 , Batch:  99 , loss avg over log interval:  3.032982347011566\n",
            "Epoch:  203 , avg total loss:  3.029494366784027\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847806186210819\n",
            "Accuracy:  0.5172011661807581\n",
            "Epoch 203/249 Completed\n",
            "Epoch:  204 , Batch:  0 , loss avg over log interval:  3.006666898727417\n",
            "Epoch:  204 , Batch:  49 , loss avg over log interval:  2.99824955701828\n",
            "Epoch:  204 , Batch:  99 , loss avg over log interval:  3.032162594795227\n",
            "Epoch:  204 , avg total loss:  3.0286078444425613\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847876944193026\n",
            "Accuracy:  0.5168124392614188\n",
            "Epoch 204/249 Completed\n",
            "Epoch:  205 , Batch:  0 , loss avg over log interval:  3.0066094398498535\n",
            "Epoch:  205 , Batch:  49 , loss avg over log interval:  2.998198673725128\n",
            "Epoch:  205 , Batch:  99 , loss avg over log interval:  3.0324191427230835\n",
            "Epoch:  205 , avg total loss:  3.028496135836062\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847749058793231\n",
            "Accuracy:  0.5170068027210885\n",
            "Epoch 205/249 Completed\n",
            "Epoch:  206 , Batch:  0 , loss avg over log interval:  2.999366044998169\n",
            "Epoch:  206 , Batch:  49 , loss avg over log interval:  2.9980076932907105\n",
            "Epoch:  206 , Batch:  99 , loss avg over log interval:  3.0320514321327208\n",
            "Epoch:  206 , avg total loss:  3.028901856014694\n",
            "iter   0\n",
            "accuracy so far =  0.4453125\n",
            "iter   0\n",
            "accuracy so far =  0.640625\n",
            "Avg Validation Loss:  6.847265167934139\n",
            "Accuracy:  0.5181729834791059\n",
            "Epoch 206/249 Completed\n",
            "Epoch:  207 , Batch:  0 , loss avg over log interval:  3.006251096725464\n",
            "Epoch:  207 , Batch:  49 , loss avg over log interval:  2.998257780075073\n",
            "Epoch:  207 , Batch:  99 , loss avg over log interval:  3.03255859375\n",
            "Epoch:  207 , avg total loss:  3.0290821877078735\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847523462481615\n",
            "Accuracy:  0.5181729834791059\n",
            "Epoch 207/249 Completed\n",
            "Epoch:  208 , Batch:  0 , loss avg over log interval:  2.9990882873535156\n",
            "Epoch:  208 , Batch:  49 , loss avg over log interval:  2.9985827946662904\n",
            "Epoch:  208 , Batch:  99 , loss avg over log interval:  3.0321945405006407\n",
            "Epoch:  208 , avg total loss:  3.0285608768463135\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.8477543272623205\n",
            "Accuracy:  0.5177842565597668\n",
            "Epoch 208/249 Completed\n",
            "Epoch:  209 , Batch:  0 , loss avg over log interval:  3.006171941757202\n",
            "Epoch:  209 , Batch:  49 , loss avg over log interval:  2.999492690563202\n",
            "Epoch:  209 , Batch:  99 , loss avg over log interval:  3.0326801538467407\n",
            "Epoch:  209 , avg total loss:  3.0293615486310874\n",
            "iter   0\n",
            "accuracy so far =  0.4453125\n",
            "iter   0\n",
            "accuracy so far =  0.6328125\n",
            "Avg Validation Loss:  6.847515908683219\n",
            "Accuracy:  0.5175898931000972\n",
            "Epoch 209/249 Completed\n",
            "Epoch:  210 , Batch:  0 , loss avg over log interval:  2.998936414718628\n",
            "Epoch:  210 , Batch:  49 , loss avg over log interval:  2.999808659553528\n",
            "Epoch:  210 , Batch:  99 , loss avg over log interval:  3.032341067790985\n",
            "Epoch:  210 , avg total loss:  3.029521942138672\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.625\n",
            "Avg Validation Loss:  6.847708411333038\n",
            "Accuracy:  0.5173955296404276\n",
            "Epoch 210/249 Completed\n",
            "Epoch:  211 , Batch:  0 , loss avg over log interval:  2.9960591793060303\n",
            "Epoch:  211 , Batch:  49 , loss avg over log interval:  2.9973892760276795\n",
            "Epoch:  211 , Batch:  99 , loss avg over log interval:  3.0336167716979983\n",
            "Epoch:  211 , avg total loss:  3.0288534950518953\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.847891743590192\n",
            "Accuracy:  0.5172011661807581\n",
            "Epoch 211/249 Completed\n",
            "Epoch:  212 , Batch:  0 , loss avg over log interval:  2.996245861053467\n",
            "Epoch:  212 , Batch:  49 , loss avg over log interval:  2.9983678364753725\n",
            "Epoch:  212 , Batch:  99 , loss avg over log interval:  3.0321968960762025\n",
            "Epoch:  212 , avg total loss:  3.028657531392747\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848152381617848\n",
            "Accuracy:  0.5170068027210885\n",
            "Epoch 212/249 Completed\n",
            "Epoch:  213 , Batch:  0 , loss avg over log interval:  2.999204158782959\n",
            "Epoch:  213 , Batch:  49 , loss avg over log interval:  2.9986237859725953\n",
            "Epoch:  213 , Batch:  99 , loss avg over log interval:  3.032092866897583\n",
            "Epoch:  213 , avg total loss:  3.0287209576454717\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848142850689772\n",
            "Accuracy:  0.5170068027210885\n",
            "Epoch 213/249 Completed\n",
            "Epoch:  214 , Batch:  0 , loss avg over log interval:  3.0061862468719482\n",
            "Epoch:  214 , Batch:  49 , loss avg over log interval:  2.997742280960083\n",
            "Epoch:  214 , Batch:  99 , loss avg over log interval:  3.0316224241256715\n",
            "Epoch:  214 , avg total loss:  3.0285230745439944\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6015625\n",
            "Avg Validation Loss:  6.848204746478942\n",
            "Accuracy:  0.5164237123420797\n",
            "Epoch 214/249 Completed\n",
            "Epoch:  215 , Batch:  0 , loss avg over log interval:  2.998894453048706\n",
            "Epoch:  215 , Batch:  49 , loss avg over log interval:  2.9981354594230654\n",
            "Epoch:  215 , Batch:  99 , loss avg over log interval:  3.031697268486023\n",
            "Epoch:  215 , avg total loss:  3.0286309503126834\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.6015625\n",
            "Avg Validation Loss:  6.848222255706787\n",
            "Accuracy:  0.5166180758017492\n",
            "Epoch 215/249 Completed\n",
            "Epoch:  216 , Batch:  0 , loss avg over log interval:  2.9989285469055176\n",
            "Epoch:  216 , Batch:  49 , loss avg over log interval:  2.9989472436904907\n",
            "Epoch:  216 , Batch:  99 , loss avg over log interval:  3.0326527333259583\n",
            "Epoch:  216 , avg total loss:  3.029030258240907\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.8482614319498945\n",
            "Accuracy:  0.5166180758017492\n",
            "Epoch 216/249 Completed\n",
            "Epoch:  217 , Batch:  0 , loss avg over log interval:  2.995993137359619\n",
            "Epoch:  217 , Batch:  49 , loss avg over log interval:  2.9982884073257448\n",
            "Epoch:  217 , Batch:  99 , loss avg over log interval:  3.032260892391205\n",
            "Epoch:  217 , avg total loss:  3.0289450374202453\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848300683789137\n",
            "Accuracy:  0.5166180758017492\n",
            "Epoch 217/249 Completed\n",
            "Epoch:  218 , Batch:  0 , loss avg over log interval:  2.9988973140716553\n",
            "Epoch:  218 , Batch:  49 , loss avg over log interval:  2.9985232305526734\n",
            "Epoch:  218 , Batch:  99 , loss avg over log interval:  3.030900523662567\n",
            "Epoch:  218 , avg total loss:  3.0282349310059478\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.847968776051591\n",
            "Accuracy:  0.5166180758017492\n",
            "Epoch 218/249 Completed\n",
            "Epoch:  219 , Batch:  0 , loss avg over log interval:  2.9956557750701904\n",
            "Epoch:  219 , Batch:  49 , loss avg over log interval:  2.99923681974411\n",
            "Epoch:  219 , Batch:  99 , loss avg over log interval:  3.032271342277527\n",
            "Epoch:  219 , avg total loss:  3.029207290082738\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.84788852203183\n",
            "Accuracy:  0.5164237123420797\n",
            "Epoch 219/249 Completed\n",
            "Epoch:  220 , Batch:  0 , loss avg over log interval:  2.9988017082214355\n",
            "Epoch:  220 , Batch:  49 , loss avg over log interval:  3.0000918817520144\n",
            "Epoch:  220 , Batch:  99 , loss avg over log interval:  3.0312299275398256\n",
            "Epoch:  220 , avg total loss:  3.0288754088291223\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848116304816269\n",
            "Accuracy:  0.5158406219630709\n",
            "Epoch 220/249 Completed\n",
            "Epoch:  221 , Batch:  0 , loss avg over log interval:  3.006072998046875\n",
            "Epoch:  221 , Batch:  49 , loss avg over log interval:  2.9988997173309326\n",
            "Epoch:  221 , Batch:  99 , loss avg over log interval:  3.0311223745346068\n",
            "Epoch:  221 , avg total loss:  3.0285352217978327\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848230443349698\n",
            "Accuracy:  0.5156462585034014\n",
            "Epoch 221/249 Completed\n",
            "Epoch:  222 , Batch:  0 , loss avg over log interval:  3.0059409141540527\n",
            "Epoch:  222 , Batch:  49 , loss avg over log interval:  2.9991411137580872\n",
            "Epoch:  222 , Batch:  99 , loss avg over log interval:  3.0312669038772584\n",
            "Epoch:  222 , avg total loss:  3.028512530568717\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848290553907069\n",
            "Accuracy:  0.5152575315840622\n",
            "Epoch 222/249 Completed\n",
            "Epoch:  223 , Batch:  0 , loss avg over log interval:  2.9956109523773193\n",
            "Epoch:  223 , Batch:  49 , loss avg over log interval:  3.0003825306892393\n",
            "Epoch:  223 , Batch:  99 , loss avg over log interval:  3.0312514758110045\n",
            "Epoch:  223 , avg total loss:  3.028982673002326\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.84826971844929\n",
            "Accuracy:  0.5156462585034014\n",
            "Epoch 223/249 Completed\n",
            "Epoch:  224 , Batch:  0 , loss avg over log interval:  2.998713254928589\n",
            "Epoch:  224 , Batch:  49 , loss avg over log interval:  2.99825199842453\n",
            "Epoch:  224 , Batch:  99 , loss avg over log interval:  3.0311611032485963\n",
            "Epoch:  224 , avg total loss:  3.028194212395212\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848163564030717\n",
            "Accuracy:  0.5158406219630709\n",
            "Epoch 224/249 Completed\n",
            "Epoch:  225 , Batch:  0 , loss avg over log interval:  3.0058271884918213\n",
            "Epoch:  225 , Batch:  49 , loss avg over log interval:  2.9985761499404906\n",
            "Epoch:  225 , Batch:  99 , loss avg over log interval:  3.0309552359580993\n",
            "Epoch:  225 , avg total loss:  3.0282143380330955\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848047000605885\n",
            "Accuracy:  0.5158406219630709\n",
            "Epoch 225/249 Completed\n",
            "Epoch:  226 , Batch:  0 , loss avg over log interval:  2.995509147644043\n",
            "Epoch:  226 , Batch:  49 , loss avg over log interval:  3.000140695571899\n",
            "Epoch:  226 , Batch:  99 , loss avg over log interval:  3.0314241862297058\n",
            "Epoch:  226 , avg total loss:  3.029239258904388\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848220482105162\n",
            "Accuracy:  0.5156462585034014\n",
            "Epoch 226/249 Completed\n",
            "Epoch:  227 , Batch:  0 , loss avg over log interval:  3.00583815574646\n",
            "Epoch:  227 , Batch:  49 , loss avg over log interval:  2.9977073812484742\n",
            "Epoch:  227 , Batch:  99 , loss avg over log interval:  3.031281955242157\n",
            "Epoch:  227 , avg total loss:  3.0278944183087004\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848308557417335\n",
            "Accuracy:  0.5154518950437318\n",
            "Epoch 227/249 Completed\n",
            "Epoch:  228 , Batch:  0 , loss avg over log interval:  2.998560905456543\n",
            "Epoch:  228 , Batch:  49 , loss avg over log interval:  2.9983152747154236\n",
            "Epoch:  228 , Batch:  99 , loss avg over log interval:  3.030886702537537\n",
            "Epoch:  228 , avg total loss:  3.0285895423612734\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848344855192231\n",
            "Accuracy:  0.5152575315840622\n",
            "Epoch 228/249 Completed\n",
            "Epoch:  229 , Batch:  0 , loss avg over log interval:  2.9953389167785645\n",
            "Epoch:  229 , Batch:  49 , loss avg over log interval:  2.9987642312049867\n",
            "Epoch:  229 , Batch:  99 , loss avg over log interval:  3.031996400356293\n",
            "Epoch:  229 , avg total loss:  3.028840236041857\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848584791509117\n",
            "Accuracy:  0.5154518950437318\n",
            "Epoch 229/249 Completed\n",
            "Epoch:  230 , Batch:  0 , loss avg over log interval:  2.9985151290893555\n",
            "Epoch:  230 , Batch:  49 , loss avg over log interval:  2.9983305764198303\n",
            "Epoch:  230 , Batch:  99 , loss avg over log interval:  3.031110827922821\n",
            "Epoch:  230 , avg total loss:  3.027958222921344\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848472519618709\n",
            "Accuracy:  0.5154518950437318\n",
            "Epoch 230/249 Completed\n",
            "Epoch:  231 , Batch:  0 , loss avg over log interval:  2.995185136795044\n",
            "Epoch:  231 , Batch:  49 , loss avg over log interval:  2.999997658729553\n",
            "Epoch:  231 , Batch:  99 , loss avg over log interval:  3.030605080127716\n",
            "Epoch:  231 , avg total loss:  3.028561372687851\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848365836027192\n",
            "Accuracy:  0.5154518950437318\n",
            "Epoch 231/249 Completed\n",
            "Epoch:  232 , Batch:  0 , loss avg over log interval:  2.9985146522521973\n",
            "Epoch:  232 , Batch:  49 , loss avg over log interval:  2.9985857462882994\n",
            "Epoch:  232 , Batch:  99 , loss avg over log interval:  3.0307553887367247\n",
            "Epoch:  232 , avg total loss:  3.0279584051906197\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848313756105377\n",
            "Accuracy:  0.5150631681243926\n",
            "Epoch 232/249 Completed\n",
            "Epoch:  233 , Batch:  0 , loss avg over log interval:  2.9986045360565186\n",
            "Epoch:  233 , Batch:  49 , loss avg over log interval:  2.9978596711158754\n",
            "Epoch:  233 , Batch:  99 , loss avg over log interval:  3.0321989917755126\n",
            "Epoch:  233 , avg total loss:  3.0284871424453845\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.84841818925811\n",
            "Accuracy:  0.5150631681243926\n",
            "Epoch 233/249 Completed\n",
            "Epoch:  234 , Batch:  0 , loss avg over log interval:  3.0057923793792725\n",
            "Epoch:  234 , Batch:  49 , loss avg over log interval:  2.9982762575149535\n",
            "Epoch:  234 , Batch:  99 , loss avg over log interval:  3.033072805404663\n",
            "Epoch:  234 , avg total loss:  3.0289248625437417\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.8483298522670095\n",
            "Accuracy:  0.5154518950437318\n",
            "Epoch 234/249 Completed\n",
            "Epoch:  235 , Batch:  0 , loss avg over log interval:  2.998603343963623\n",
            "Epoch:  235 , Batch:  49 , loss avg over log interval:  2.998489501476288\n",
            "Epoch:  235 , Batch:  99 , loss avg over log interval:  3.030275647640228\n",
            "Epoch:  235 , avg total loss:  3.028199311615764\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848243027198605\n",
            "Accuracy:  0.5152575315840622\n",
            "Epoch 235/249 Completed\n",
            "Epoch:  236 , Batch:  0 , loss avg over log interval:  2.9952285289764404\n",
            "Epoch:  236 , Batch:  49 , loss avg over log interval:  3.0001050543785097\n",
            "Epoch:  236 , Batch:  99 , loss avg over log interval:  3.031094825267792\n",
            "Epoch:  236 , avg total loss:  3.0288953642914263\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848401267354081\n",
            "Accuracy:  0.5150631681243926\n",
            "Epoch 236/249 Completed\n",
            "Epoch:  237 , Batch:  0 , loss avg over log interval:  2.998473644256592\n",
            "Epoch:  237 , Batch:  49 , loss avg over log interval:  2.9992004108428953\n",
            "Epoch:  237 , Batch:  99 , loss avg over log interval:  3.0308992958068846\n",
            "Epoch:  237 , avg total loss:  3.0281583813653477\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848161307776847\n",
            "Accuracy:  0.5154518950437318\n",
            "Epoch 237/249 Completed\n",
            "Epoch:  238 , Batch:  0 , loss avg over log interval:  2.9986140727996826\n",
            "Epoch:  238 , Batch:  49 , loss avg over log interval:  2.99711345911026\n",
            "Epoch:  238 , Batch:  99 , loss avg over log interval:  3.0324327659606936\n",
            "Epoch:  238 , avg total loss:  3.0287061946979468\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848108954545928\n",
            "Accuracy:  0.5156462585034014\n",
            "Epoch 238/249 Completed\n",
            "Epoch:  239 , Batch:  0 , loss avg over log interval:  2.998608112335205\n",
            "Epoch:  239 , Batch:  49 , loss avg over log interval:  2.9979185318946837\n",
            "Epoch:  239 , Batch:  99 , loss avg over log interval:  3.031930286884308\n",
            "Epoch:  239 , avg total loss:  3.0283201794693437\n",
            "iter   0\n",
            "accuracy so far =  0.4375\n",
            "iter   0\n",
            "accuracy so far =  0.609375\n",
            "Avg Validation Loss:  6.848101435638055\n",
            "Accuracy:  0.5156462585034014\n",
            "Epoch 239/249 Completed\n",
            "Epoch:  240 , Batch:  0 , loss avg over log interval:  2.995581865310669\n",
            "Epoch:  240 , Batch:  49 , loss avg over log interval:  2.9978889846801757\n",
            "Epoch:  240 , Batch:  99 , loss avg over log interval:  3.031627504825592\n",
            "Epoch:  240 , avg total loss:  3.028686417185742\n",
            "iter   0\n",
            "accuracy so far =  0.4453125\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.847909654059062\n",
            "Accuracy:  0.5158406219630709\n",
            "Epoch 240/249 Completed\n",
            "Epoch:  241 , Batch:  0 , loss avg over log interval:  2.9956092834472656\n",
            "Epoch:  241 , Batch:  49 , loss avg over log interval:  2.9988285636901857\n",
            "Epoch:  241 , Batch:  99 , loss avg over log interval:  3.0314285588264465\n",
            "Epoch:  241 , avg total loss:  3.028560971868211\n",
            "iter   0\n",
            "accuracy so far =  0.4453125\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.847890731765003\n",
            "Accuracy:  0.5160349854227405\n",
            "Epoch 241/249 Completed\n",
            "Epoch:  242 , Batch:  0 , loss avg over log interval:  3.0057637691497803\n",
            "Epoch:  242 , Batch:  49 , loss avg over log interval:  2.9983696794509886\n",
            "Epoch:  242 , Batch:  99 , loss avg over log interval:  3.030606746673584\n",
            "Epoch:  242 , avg total loss:  3.0280012168746064\n",
            "iter   0\n",
            "accuracy so far =  0.4453125\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.847693274660808\n",
            "Accuracy:  0.5160349854227405\n",
            "Epoch 242/249 Completed\n",
            "Epoch:  243 , Batch:  0 , loss avg over log interval:  2.995506763458252\n",
            "Epoch:  243 , Batch:  49 , loss avg over log interval:  2.9981426906585695\n",
            "Epoch:  243 , Batch:  99 , loss avg over log interval:  3.030938584804535\n",
            "Epoch:  243 , avg total loss:  3.0280705794044165\n",
            "iter   0\n",
            "accuracy so far =  0.4453125\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.847645288560448\n",
            "Accuracy:  0.5160349854227405\n",
            "Epoch 243/249 Completed\n",
            "Epoch:  244 , Batch:  0 , loss avg over log interval:  3.00569486618042\n",
            "Epoch:  244 , Batch:  49 , loss avg over log interval:  2.9974072122573854\n",
            "Epoch:  244 , Batch:  99 , loss avg over log interval:  3.0307791137695315\n",
            "Epoch:  244 , avg total loss:  3.027765887371008\n",
            "iter   0\n",
            "accuracy so far =  0.4453125\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.8479204119705575\n",
            "Accuracy:  0.5156462585034014\n",
            "Epoch 244/249 Completed\n",
            "Epoch:  245 , Batch:  0 , loss avg over log interval:  2.9986767768859863\n",
            "Epoch:  245 , Batch:  49 , loss avg over log interval:  2.9975431060791013\n",
            "Epoch:  245 , Batch:  99 , loss avg over log interval:  3.030727620124817\n",
            "Epoch:  245 , avg total loss:  3.027785495571468\n",
            "iter   0\n",
            "accuracy so far =  0.4453125\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.848194634042135\n",
            "Accuracy:  0.5154518950437318\n",
            "Epoch 245/249 Completed\n",
            "Epoch:  246 , Batch:  0 , loss avg over log interval:  2.9954819679260254\n",
            "Epoch:  246 , Batch:  49 , loss avg over log interval:  2.998247134685516\n",
            "Epoch:  246 , Batch:  99 , loss avg over log interval:  3.0308867573738096\n",
            "Epoch:  246 , avg total loss:  3.0281717276227647\n",
            "iter   0\n",
            "accuracy so far =  0.4453125\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.848262478665608\n",
            "Accuracy:  0.5154518950437318\n",
            "Epoch 246/249 Completed\n",
            "Epoch:  247 , Batch:  0 , loss avg over log interval:  2.9954910278320312\n",
            "Epoch:  247 , Batch:  49 , loss avg over log interval:  2.9985404229164123\n",
            "Epoch:  247 , Batch:  99 , loss avg over log interval:  3.03188942193985\n",
            "Epoch:  247 , avg total loss:  3.028461704219597\n",
            "iter   0\n",
            "accuracy so far =  0.4453125\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.848470024946259\n",
            "Accuracy:  0.5156462585034014\n",
            "Epoch 247/249 Completed\n",
            "Epoch:  248 , Batch:  0 , loss avg over log interval:  3.0057382583618164\n",
            "Epoch:  248 , Batch:  49 , loss avg over log interval:  2.997914776802063\n",
            "Epoch:  248 , Batch:  99 , loss avg over log interval:  3.031515486240387\n",
            "Epoch:  248 , avg total loss:  3.0284491475077644\n",
            "iter   0\n",
            "accuracy so far =  0.4453125\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.848729098715434\n",
            "Accuracy:  0.5152575315840622\n",
            "Epoch 248/249 Completed\n",
            "Epoch:  249 , Batch:  0 , loss avg over log interval:  2.995588541030884\n",
            "Epoch:  249 , Batch:  49 , loss avg over log interval:  2.9978508949279785\n",
            "Epoch:  249 , Batch:  99 , loss avg over log interval:  3.0303344440460207\n",
            "Epoch:  249 , avg total loss:  3.0278478809024976\n",
            "iter   0\n",
            "accuracy so far =  0.4453125\n",
            "iter   0\n",
            "accuracy so far =  0.6171875\n",
            "Avg Validation Loss:  6.848721091340228\n",
            "Accuracy:  0.5150631681243926\n",
            "Epoch 249/249 Completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDmKw9pMH3NJ"
      },
      "source": [
        "# Resume Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UlgNih0H2DD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "69ee53f5-b948-4670-9fdf-d3b116afac47"
      },
      "source": [
        "EPOCH_SAVE_PREFIX = '/content/drive/Shared drives/CIS680 Final Project/models/fused_sampled_3_relu_rahul_sgd/'\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "torch.random.manual_seed(1)\n",
        "\n",
        "# load trained spatial\n",
        "spatial = SpatialStream()\n",
        "spatial.to(device)\n",
        "spatial_network_path = '/content/drive/Shared drives/CIS680 Final Project/models/spatial_sampled_3/spatial_epoch445'\n",
        "checkpoint_spatial = torch.load(spatial_network_path)\n",
        "spatial.load_state_dict(checkpoint_spatial['model_state_dict'])\n",
        "\n",
        "# load trained temporal\n",
        "temporal = TemporalStream()\n",
        "temporal.to(device)\n",
        "temporal_network_path = '/content/drive/Shared drives/CIS680 Final Project/models/temporal_new_data/temporal_epoch288'\n",
        "checkpoint_temporal = torch.load(temporal_network_path)\n",
        "temporal.load_state_dict(checkpoint_temporal['model_state_dict'])\n",
        "\n",
        "\n",
        "# fused model\n",
        "learning_rate = 0.001\n",
        "fused = FuseNET()\n",
        "fused.to(device)\n",
        "optimizer = optim.SGD(fused.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "# Epochs\n",
        "num_epochs = 1000\n",
        "batch_size = 64\n",
        "epoch_list = np.arange(num_epochs)\n",
        "\n",
        "\n",
        "# LOAD NETWORK\n",
        "fused_network_path = EPOCH_SAVE_PREFIX + 'fused_sampled_epoch7'\n",
        "checkpoint_fused = torch.load(fused_network_path)\n",
        "fused.load_state_dict(checkpoint_fused['model_state_dict'])\n",
        "#optimizer.load_state_dict(checkpoint_fused['optimizer_state_dict'])\n",
        "last_epoch = checkpoint_fused['epoch']\n",
        "\n",
        "# Logging setup: train\n",
        "train_loss_list = checkpoint_fused['train_total_loss_list']\n",
        "epoch_loss_list = checkpoint_fused['epoch_total_loss_list']\n",
        "\n",
        "test_loss_list = checkpoint_fused['test_loss_list']\n",
        "accuracy_list = checkpoint_fused['accuracy_list']\n",
        "train_counter = checkpoint_fused['train_counter']\n",
        "\n",
        "\n",
        "# epoch loop\n",
        "for epoch in range(last_epoch + 1, num_epochs):\n",
        "\n",
        "    # Train & Validate\n",
        "    train(epoch)\n",
        "    test()\n",
        "\n",
        "    # Save Model Version\n",
        "    save_path = os.path.join(EPOCH_SAVE_PREFIX, 'fused_sampled_epoch' + str(epoch))\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'train_total_loss_list': train_loss_list,\n",
        "        'epoch_total_loss_list': epoch_loss_list,\n",
        "        'test_loss_list': test_loss_list,\n",
        "        'train_counter': train_counter,\n",
        "        'accuracy_list': accuracy_list,\n",
        "        'model_state_dict': fused.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, save_path)\n",
        "\n",
        "    print(\"Epoch %d/%d Completed\" % (epoch, num_epochs - 1))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  8 , Batch:  0 , loss avg over log interval:  3.017577648162842\n",
            "Epoch:  8 , Batch:  49 , loss avg over log interval:  3.016844239234924\n",
            "Epoch:  8 , Batch:  99 , loss avg over log interval:  3.0447605061531067\n",
            "Epoch:  8 , avg total loss:  3.044150179710941\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.836523788731273\n",
            "Accuracy:  0.506859756097561\n",
            "Epoch 8/999 Completed\n",
            "Epoch:  9 , Batch:  0 , loss avg over log interval:  3.018711566925049\n",
            "Epoch:  9 , Batch:  49 , loss avg over log interval:  3.013567569255829\n",
            "Epoch:  9 , Batch:  99 , loss avg over log interval:  3.0438222742080687\n",
            "Epoch:  9 , avg total loss:  3.042521598546401\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.836684994581269\n",
            "Accuracy:  0.5072408536585366\n",
            "Epoch 9/999 Completed\n",
            "Epoch:  10 , Batch:  0 , loss avg over log interval:  3.01759934425354\n",
            "Epoch:  10 , Batch:  49 , loss avg over log interval:  3.0166345405578614\n",
            "Epoch:  10 , Batch:  99 , loss avg over log interval:  3.043738601207733\n",
            "Epoch:  10 , avg total loss:  3.0435922439547554\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.8367550663831755\n",
            "Accuracy:  0.5070503048780488\n",
            "Epoch 10/999 Completed\n",
            "Epoch:  11 , Batch:  0 , loss avg over log interval:  3.0187313556671143\n",
            "Epoch:  11 , Batch:  49 , loss avg over log interval:  3.015747151374817\n",
            "Epoch:  11 , Batch:  99 , loss avg over log interval:  3.043773181438446\n",
            "Epoch:  11 , avg total loss:  3.0432810895684836\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.837027799792406\n",
            "Accuracy:  0.506859756097561\n",
            "Epoch 11/999 Completed\n",
            "Epoch:  12 , Batch:  0 , loss avg over log interval:  3.0176210403442383\n",
            "Epoch:  12 , Batch:  49 , loss avg over log interval:  3.0160541343688965\n",
            "Epoch:  12 , Batch:  99 , loss avg over log interval:  3.0435967350006106\n",
            "Epoch:  12 , avg total loss:  3.043157564557117\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.837167658456942\n",
            "Accuracy:  0.5066692073170732\n",
            "Epoch 12/999 Completed\n",
            "Epoch:  13 , Batch:  0 , loss avg over log interval:  3.0176267623901367\n",
            "Epoch:  13 , Batch:  49 , loss avg over log interval:  3.016397352218628\n",
            "Epoch:  13 , Batch:  99 , loss avg over log interval:  3.045192873477936\n",
            "Epoch:  13 , avg total loss:  3.044400362000949\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.837082606990163\n",
            "Accuracy:  0.506859756097561\n",
            "Epoch 13/999 Completed\n",
            "Epoch:  14 , Batch:  0 , loss avg over log interval:  3.01874041557312\n",
            "Epoch:  14 , Batch:  49 , loss avg over log interval:  3.01615802526474\n",
            "Epoch:  14 , Batch:  99 , loss avg over log interval:  3.0436141300201416\n",
            "Epoch:  14 , avg total loss:  3.0435821077098018\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.837217662392593\n",
            "Accuracy:  0.5062881097560976\n",
            "Epoch 14/999 Completed\n",
            "Epoch:  15 , Batch:  0 , loss avg over log interval:  3.0176198482513428\n",
            "Epoch:  15 , Batch:  49 , loss avg over log interval:  3.0157810282707214\n",
            "Epoch:  15 , Batch:  99 , loss avg over log interval:  3.043882746696472\n",
            "Epoch:  15 , avg total loss:  3.0433671249859575\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.837086305385682\n",
            "Accuracy:  0.5066692073170732\n",
            "Epoch 15/999 Completed\n",
            "Epoch:  16 , Batch:  0 , loss avg over log interval:  3.018738031387329\n",
            "Epoch:  16 , Batch:  49 , loss avg over log interval:  3.0154316306114195\n",
            "Epoch:  16 , Batch:  99 , loss avg over log interval:  3.044439580440521\n",
            "Epoch:  16 , avg total loss:  3.0436070656430894\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.837131046667332\n",
            "Accuracy:  0.5064786585365854\n",
            "Epoch 16/999 Completed\n",
            "Epoch:  17 , Batch:  0 , loss avg over log interval:  3.0187366008758545\n",
            "Epoch:  17 , Batch:  49 , loss avg over log interval:  3.0150100684165952\n",
            "Epoch:  17 , Batch:  99 , loss avg over log interval:  3.044277970790863\n",
            "Epoch:  17 , avg total loss:  3.0429403894189475\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.83694323679296\n",
            "Accuracy:  0.5066692073170732\n",
            "Epoch 17/999 Completed\n",
            "Epoch:  18 , Batch:  0 , loss avg over log interval:  3.0175528526306152\n",
            "Epoch:  18 , Batch:  49 , loss avg over log interval:  3.016781666278839\n",
            "Epoch:  18 , Batch:  99 , loss avg over log interval:  3.0421742367744447\n",
            "Epoch:  18 , avg total loss:  3.043386825616809\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.837149503754406\n",
            "Accuracy:  0.5062881097560976\n",
            "Epoch 18/999 Completed\n",
            "Epoch:  19 , Batch:  0 , loss avg over log interval:  3.017578363418579\n",
            "Epoch:  19 , Batch:  49 , loss avg over log interval:  3.015557553768158\n",
            "Epoch:  19 , Batch:  99 , loss avg over log interval:  3.0446868085861207\n",
            "Epoch:  19 , avg total loss:  3.043771721314693\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.837331515986745\n",
            "Accuracy:  0.5059070121951219\n",
            "Epoch 19/999 Completed\n",
            "Epoch:  20 , Batch:  0 , loss avg over log interval:  3.024778366088867\n",
            "Epoch:  20 , Batch:  49 , loss avg over log interval:  3.0156861996650695\n",
            "Epoch:  20 , Batch:  99 , loss avg over log interval:  3.0426712250709533\n",
            "Epoch:  20 , avg total loss:  3.0432280466176462\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.837389730825657\n",
            "Accuracy:  0.5059070121951219\n",
            "Epoch 20/999 Completed\n",
            "Epoch:  21 , Batch:  0 , loss avg over log interval:  3.018742084503174\n",
            "Epoch:  21 , Batch:  49 , loss avg over log interval:  3.01721271276474\n",
            "Epoch:  21 , Batch:  99 , loss avg over log interval:  3.0435475063323976\n",
            "Epoch:  21 , avg total loss:  3.043939494568369\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.8372604090993\n",
            "Accuracy:  0.5060975609756098\n",
            "Epoch 21/999 Completed\n",
            "Epoch:  22 , Batch:  0 , loss avg over log interval:  3.0247786045074463\n",
            "Epoch:  22 , Batch:  49 , loss avg over log interval:  3.016376531124115\n",
            "Epoch:  22 , Batch:  99 , loss avg over log interval:  3.0449424839019774\n",
            "Epoch:  22 , avg total loss:  3.043980683969415\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.837505317315823\n",
            "Accuracy:  0.5059070121951219\n",
            "Epoch 22/999 Completed\n",
            "Epoch:  23 , Batch:  0 , loss avg over log interval:  3.0187535285949707\n",
            "Epoch:  23 , Batch:  49 , loss avg over log interval:  3.0159626388549805\n",
            "Epoch:  23 , Batch:  99 , loss avg over log interval:  3.043289530277252\n",
            "Epoch:  23 , avg total loss:  3.043369904808376\n",
            "iter   0\n",
            "accuracy so far =  0.953125\n",
            "iter   0\n",
            "accuracy so far =  1.40625\n",
            "Avg Validation Loss:  6.83774398012859\n",
            "Accuracy:  0.5057164634146342\n",
            "Epoch 23/999 Completed\n",
            "Epoch:  24 , Batch:  0 , loss avg over log interval:  3.0248258113861084\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-8c4fe9659115>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Train & Validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-756ac2c35082>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"indexes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mSG3I\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSG3I\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mSG3I\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSG3I\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mSG3I\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSG3I\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-a8481dd482eb>\u001b[0m in \u001b[0;36mgetSG3I\u001b[0;34m(videos)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mframe_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mframe_list_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWSTqvq_FBZb"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q4XGcuDFCLX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "outputId": "7390e7bb-9803-4d9f-967f-13dcee7cc298"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "EPOCH_SAVE_PREFIX = '/content/drive/Shared drives/CIS680 Final Project/models/fused_sampled_3_relu_rahul_sgd/'\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "torch.random.manual_seed(1)\n",
        "\n",
        "# Load params\n",
        "last_epoch = 14\n",
        "\n",
        "# load data\n",
        "network_path = EPOCH_SAVE_PREFIX + 'fused_sampled_epoch' + str(last_epoch)\n",
        "checkpoint = torch.load(network_path)\n",
        "train_loss_list = checkpoint['train_total_loss_list']\n",
        "epoch_loss_list = checkpoint['epoch_total_loss_list']\n",
        "train_counter = checkpoint['train_counter']\n",
        "test_loss_list = checkpoint['test_loss_list']\n",
        "accuracy_list = checkpoint['accuracy_list']\n",
        "epoch_list = np.arange(last_epoch+1)\n",
        "\n",
        "\n",
        "# plots\n",
        "fig = plt.figure()\n",
        "plt.plot(epoch_loss_list, color='blue')\n",
        "plt.legend(['FuseNet Train Loss'], loc='upper right')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Total Loss')\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(test_loss_list, color='green')\n",
        "plt.legend(['FuseNet Validation Loss'], loc='upper right')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Total Loss')\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(accuracy_list, color='red')\n",
        "plt.legend(['FuseNet Validation Accuracy'], loc='lower right')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}